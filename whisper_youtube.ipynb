{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Youtube Videos Transcription with OpenAI's Whisper**\n",
        "\n",
        "[![blog post shield](https://img.shields.io/static/v1?label=&message=Blog%20post&color=blue&style=for-the-badge&logo=openai&link=https://openai.com/blog/whisper)](https://openai.com/blog/whisper)\n",
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/openai/whisper)\n",
        "[![paper shield](https://img.shields.io/static/v1?label=&message=Paper&color=blue&style=for-the-badge&link=https://cdn.openai.com/papers/whisper.pdf)](https://cdn.openai.com/papers/whisper.pdf)\n",
        "[![model card shield](https://img.shields.io/static/v1?label=&message=Model%20card&color=blue&style=for-the-badge&link=https://github.com/openai/whisper/blob/main/model-card.md)](https://github.com/openai/whisper/blob/main/model-card.md)\n",
        "\n",
        "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
        "\n",
        "This Notebook will guide you through the transcription of a Youtube video using Whisper. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript and video audio in your Google Drive."
      ],
      "metadata": {
        "id": "96kvih9mXkNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Check GPU type** 🕵️\n",
        "\n",
        "#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n",
        "#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n",
        "#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n",
        "#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "QshUbLqpX7L4",
        "outputId": "a4cc6c03-0638-4558-c8ed-746bb3c07f53"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-2e665f60-f1d2-4ac7-c0c5-9fa8853d5b3c)\n",
            "Sun Oct  2 21:07:26 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    31W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfG0E_WbRFI0",
        "outputId": "6316d142-41cf-41f1-8018-cdb0208f175b",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-wpki4m4p\n",
            "  Running command git clone -q https://github.com/openai/whisper.git /tmp/pip-req-build-wpki4m4p\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (8.14.0)\n",
            "Requirement already satisfied: transformers>=4.19.0 in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (4.22.2)\n",
            "Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.7/dist-packages (from whisper==1.0) (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python==0.2.0->whisper==1.0) (0.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (0.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.19.0->whisper==1.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers>=4.19.0->whisper==1.0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.19.0->whisper==1.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.19.0->whisper==1.0) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.19.0->whisper==1.0) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.7/dist-packages (12.1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Install libraries** 🏗️\n",
        "#@markdown This cell will take a little while to download several libraries, including Whisper.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install pytube\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "import pytube\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1zwGAsr4sIgd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "338e0e6e-e903-4661-8cd8-6a613ce169ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Optional:** Save data in Google Drive 💾\n",
        "#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "from google.colab import drive\n",
        "drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"My Drive\"\n",
        "#@markdown ---\n",
        "drive_path = \"Colab Notebooks/Whisper Youtube\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change your Google Drive path.**\n",
        "\n",
        "drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n",
        "drive_whisper_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Model selection** 🧠\n",
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "Model = 'large' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is selected.**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.**<br /> Please select one of the following:<br /> - {'<br /> - '.join(whisper.available_models())}\"\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "cellView": "form",
        "id": "TMhrSq_GZ6kA",
        "outputId": "8db5f89b-4caa-464b-9f6c-051985d001e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**large model is selected.**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Video selection** 📺\n",
        "\n",
        "#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
        "\n",
        "#@markdown ---\n",
        "URL = \"https://youtu.be/bIrEM2FbOLU\" #@param {type:\"string\"}\n",
        "store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**\n",
        "\n",
        "video_yt = pytube.YouTube(URL)\n",
        "\n",
        "try:\n",
        "    video_yt.check_availability()\n",
        "    display(\n",
        "        YouTubeVideo(video_yt.video_id)\n",
        "    )\n",
        "except pytube.exceptions.VideoUnavailable:\n",
        "    display(\n",
        "        Markdown(f\"**{URL} isn't available.**\"),\n",
        "    )\n",
        "\n",
        "\n",
        "video_path_local = Path(\".\").resolve() / (video_yt.video_id+\".wav\")\n",
        "video_yt.streams.filter(\n",
        "    type=\"audio\",\n",
        "    mime_type=\"audio/mp4\",\n",
        "    abr=\"48kbps\"\n",
        ").first().download(\n",
        "    output_path = video_path_local.parent,\n",
        "    filename = video_path_local.name\n",
        ")\n",
        "\n",
        "result = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n",
        "\n",
        "if store_audio:\n",
        "    shutil.copy(video_path_local, drive_whisper_path / video_path_local.name)"
      ],
      "metadata": {
        "id": "xYLPZQX9S7tU",
        "outputId": "51dee1e1-37f5-4840-cfd2-a31848e47353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7fc78fd0f0d0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/bIrEM2FbOLU\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-X0qB9JAzMLY",
        "outputId": "b4528826-b2eb-4e6c-b94a-225edd2fc993",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:05.360]  The following is a conversation with Greg Brockman. He's the co-founder and CTO of OpenAI,\n",
            "[00:05.360 --> 00:11.520]  a world-class research organization developing ideas in AI with a goal of eventually creating a\n",
            "[00:11.520 --> 00:18.160]  safe and friendly artificial general intelligence, one that benefits and empowers humanity.\n",
            "[00:18.880 --> 00:25.600]  OpenAI is not only a source of publications, algorithms, tools, and datasets. Their mission\n",
            "[00:25.600 --> 00:32.240]  is a catalyst for an important public discourse about our future with both narrow and general\n",
            "[00:32.240 --> 00:37.440]  intelligence systems. This conversation is part of the Artificial Intelligence podcast\n",
            "[00:37.440 --> 00:43.920]  at MIT and beyond. If you enjoy it, subscribe on YouTube, iTunes, or simply connect with me on\n",
            "[00:43.920 --> 00:51.680]  Twitter at Lex Friedman, spelled F-R-I-D. And now, here's my conversation with Greg Brockman.\n",
            "[00:51.680 --> 00:57.360]  So in high school and right after you wrote a draft of a chemistry textbook, I saw that. That\n",
            "[00:57.360 --> 01:03.040]  covers everything from basic structure of the atom to quantum mechanics. So it's clear you have an\n",
            "[01:03.040 --> 01:10.240]  intuition and a passion for both the physical world with chemistry and now robotics to the\n",
            "[01:10.240 --> 01:16.400]  digital world with AI, deep learning, reinforcement learning, and so on. Do you see the physical world\n",
            "[01:16.400 --> 01:19.760]  and the digital world as different? And what do you think is the gap?\n",
            "[01:19.760 --> 01:25.040]  A lot of it actually boils down to iteration speed. I think that a lot of what really motivates me is\n",
            "[01:25.040 --> 01:29.760]  building things. Think about mathematics, for example, where you think really hard about a\n",
            "[01:29.760 --> 01:33.520]  problem, you understand it. You write it down in this very obscure form that we call proof.\n",
            "[01:34.000 --> 01:38.720]  But then this is in humanity's library. It's there forever. This is some truth that we've\n",
            "[01:38.720 --> 01:43.200]  discovered. Maybe only five people in your field will ever read it, but somehow you've kind of\n",
            "[01:43.200 --> 01:47.040]  moved humanity forward. And so I actually used to really think that I was going to be a\n",
            "[01:47.040 --> 01:51.760]  mathematician. And then I actually started writing this chemistry textbook. One of my friends told\n",
            "[01:51.760 --> 01:57.440]  me, you'll never publish it because you don't have a PhD. So instead, I decided to build a website\n",
            "[01:57.440 --> 02:02.960]  and try to promote my ideas that way. And then I discovered programming. And you know that in\n",
            "[02:02.960 --> 02:07.200]  programming, you think hard about a problem, you understand it, you write down in a very obscure\n",
            "[02:07.200 --> 02:12.560]  form that we call a program. But then once again, it's in humanity's library, right? And anyone can\n",
            "[02:12.560 --> 02:16.480]  get the benefit from it. And the scalability is massive. And so I think that the thing that\n",
            "[02:16.480 --> 02:20.320]  the thing that really appeals to me about the digital world is that you can have this this\n",
            "[02:20.320 --> 02:25.680]  this insane leverage, right? A single individual with an idea is able to affect the entire planet.\n",
            "[02:26.240 --> 02:30.240]  And that's something I think is really hard to do if you're moving around physical atoms.\n",
            "[02:30.240 --> 02:37.680]  But you said mathematics. So if you look at the wet thing over here, our mind, do you ultimately\n",
            "[02:37.680 --> 02:44.320]  see it as just math is just information processing? Or is there some other magic? As you've seen,\n",
            "[02:44.320 --> 02:48.000]  if you've seen through biology and chemistry, and so on? I think it's really interesting to think\n",
            "[02:48.000 --> 02:52.640]  about humans as just information processing systems. And that seems like it's actually a\n",
            "[02:52.640 --> 02:58.400]  pretty good way of describing a lot of kind of how the world works, or a lot of what we're capable of\n",
            "[02:58.400 --> 03:03.600]  to think that that, you know, again, if you just look at technological innovations over time,\n",
            "[03:03.600 --> 03:06.640]  that in some ways, the most transformative innovation that we've had is has been the\n",
            "[03:06.640 --> 03:10.560]  computer, right? In some ways, the internet, you know, that what is the internet done, right? The\n",
            "[03:10.560 --> 03:15.280]  internet is not about these physical cables. It's about the fact that I am suddenly able to instantly\n",
            "[03:15.280 --> 03:20.320]  communicate with any other human on the planet. I'm able to retrieve any piece of knowledge that\n",
            "[03:20.320 --> 03:26.000]  in some ways, the human race has ever had. And that those are these insane transformations.\n",
            "[03:26.000 --> 03:30.640]  Do you see the our society as a whole, the collective as another extension of the\n",
            "[03:30.640 --> 03:34.960]  intelligence of the human being? So if you look at the human beings, information processing system,\n",
            "[03:34.960 --> 03:39.360]  you mentioned the internet, the networking, do you see us all together as a civilization,\n",
            "[03:39.360 --> 03:43.520]  as a as a kind of intelligent system? Yeah, I think this is actually a really interesting\n",
            "[03:43.520 --> 03:48.480]  perspective to take into think about that you sort of have this collective intelligence of\n",
            "[03:48.480 --> 03:54.080]  all of society, the economy itself is the superhuman machine that is optimizing something,\n",
            "[03:54.080 --> 03:58.480]  right? And it's almost some ways, a company has a will of its own, right, that you have all these\n",
            "[03:58.480 --> 04:01.920]  individuals who are all pursuing their own individual goals and thinking really hard,\n",
            "[04:01.920 --> 04:06.560]  and thinking about the right things to do. But somehow the company does something that is this\n",
            "[04:06.560 --> 04:12.160]  emergent thing. And that is a really useful abstraction. And so I think that in some ways,\n",
            "[04:12.160 --> 04:15.840]  you know, we think of ourselves as the most intelligent things on the planet, and the most\n",
            "[04:15.840 --> 04:19.600]  powerful things on the planet. But there are things that are bigger than us that are these\n",
            "[04:19.600 --> 04:23.760]  systems that we all contribute to. And so I think actually, you know, it's a it's interesting to\n",
            "[04:23.760 --> 04:28.720]  think about, if you've read Isaac Aizamov's foundation, right, that there's this concept\n",
            "[04:28.720 --> 04:32.480]  of psychohistory in there, which is effectively this, that if you have trillions or quadrillions\n",
            "[04:32.480 --> 04:38.080]  of beings, then maybe you could actually predict what that being that huge macro being will do,\n",
            "[04:39.040 --> 04:44.160]  and almost independent of what the individuals want. And I actually have a second angle on this\n",
            "[04:44.160 --> 04:48.960]  that I think is interesting, which is thinking about technological determinism. One thing that\n",
            "[04:48.960 --> 04:54.240]  I actually think a lot about with with OpenAI, right, is that we're kind of coming on onto this\n",
            "[04:54.240 --> 04:58.160]  insanely transformational technology of general intelligence, right, that will happen at some\n",
            "[04:58.160 --> 05:03.280]  point. And there's a question of how can you take actions that will actually steer it to go better\n",
            "[05:03.280 --> 05:08.400]  rather than worse. And that I think one question you need to ask is, as a scientist, as an inventor,\n",
            "[05:08.400 --> 05:12.800]  as a creator, what impact can you have in general, right? You look at things like the telephone\n",
            "[05:12.800 --> 05:16.880]  invented by two people on the same day. Like, what does that mean? Like, what does that mean about the\n",
            "[05:16.880 --> 05:20.880]  shape of innovation? And I think that what's going on is everyone's building on the shoulders of the\n",
            "[05:20.880 --> 05:25.200]  same giants. And so you can kind of, you can't really hope to create something no one else ever\n",
            "[05:25.200 --> 05:29.280]  would. You know, if Einstein wasn't born, someone else would have come up with relativity. You know,\n",
            "[05:29.280 --> 05:32.880]  he changed the timeline a bit, right, that maybe it would have taken another 20 years.\n",
            "[05:32.880 --> 05:37.280]  But it wouldn't be that fundamentally, humanity would never discover these fundamental truths.\n",
            "[05:37.280 --> 05:44.080]  So there's some kind of invisible momentum that some people like Einstein or OpenAI is plugging\n",
            "[05:44.080 --> 05:50.320]  into that anybody else can also plug into. And ultimately, that wave takes us into a certain\n",
            "[05:50.320 --> 05:53.120]  direction. That's what you mean by determinism. That's right. That's right. And, you know,\n",
            "[05:53.120 --> 05:56.880]  this kind of seems to play out in a bunch of different ways, that there's some exponential\n",
            "[05:56.880 --> 06:00.960]  that is being written, and that the exponential itself, which one it is, changes. Think about\n",
            "[06:00.960 --> 06:06.160]  Moore's law. An entire industry set its clock to it for 50 years. Like, how can that be? Right?\n",
            "[06:06.160 --> 06:11.840]  How is that possible? And yet somehow it happened. And so I think you can't hope to ever invent\n",
            "[06:11.840 --> 06:16.160]  something that no one else will. Maybe you can change the timeline a little bit. But if you\n",
            "[06:16.160 --> 06:20.080]  really want to make a difference, I think that the thing that you really have to do, the only\n",
            "[06:20.080 --> 06:24.000]  real degree of freedom you have is to set the initial conditions under which a technology is\n",
            "[06:24.000 --> 06:28.000]  born. And so you think about the internet, right? That there are lots of other competitors trying\n",
            "[06:28.000 --> 06:33.920]  to build similar things. And the internet won. And the initial conditions were that it was created\n",
            "[06:33.920 --> 06:38.960]  by this group that really valued people being able to be, you know, anyone being able to plug\n",
            "[06:38.960 --> 06:43.760]  in this very academic mindset of being open and connected. And I think that the internet for the\n",
            "[06:43.760 --> 06:48.720]  next 40 years really played out that way. You know, maybe today things are starting to shift\n",
            "[06:48.720 --> 06:52.000]  in a different direction. But I think that those initial conditions were really important to\n",
            "[06:52.000 --> 06:57.200]  determine the next 40 years worth of progress. That's really beautifully put. So another example\n",
            "[06:57.200 --> 07:02.320]  of that I think about, you know, I recently looked at it. I looked at Wikipedia, the formation of\n",
            "[07:02.320 --> 07:08.240]  Wikipedia. And I wonder what the internet would be like if Wikipedia had ads. You know, there's\n",
            "[07:08.240 --> 07:14.480]  an interesting argument that why they chose not to make it, put advertisement on Wikipedia. I think\n",
            "[07:14.480 --> 07:19.760]  it's, I think Wikipedia is one of the greatest resources we have on the internet. It's extremely\n",
            "[07:19.760 --> 07:25.120]  surprising how well it works and how well it was able to aggregate all this kind of good information.\n",
            "[07:25.120 --> 07:29.280]  And essentially the creator Wikipedia, I don't know, there's probably some debates there,\n",
            "[07:29.280 --> 07:34.080]  but set the initial conditions and now it carried itself forward. That's really interesting. So\n",
            "[07:34.080 --> 07:38.560]  you're, the way you're thinking about AGI or artificial intelligence is you're focused on\n",
            "[07:38.560 --> 07:44.240]  setting the initial conditions for the progress. That's right. That's powerful. Okay. So looking\n",
            "[07:44.240 --> 07:50.880]  to the future, if you create an AGI system, like one that can ace the Turing test, natural language,\n",
            "[07:51.600 --> 07:56.560]  what do you think would be the interactions you would have with it? What do you think are\n",
            "[07:56.560 --> 08:01.760]  the questions you would ask? Like what would be the first question you would ask it, her, him?\n",
            "[08:01.760 --> 08:06.400]  That's right. I think that at that point, if you've really built a powerful system that\n",
            "[08:06.400 --> 08:10.160]  is capable of shaping the future of humanity, the first question that you really should ask\n",
            "[08:10.160 --> 08:14.320]  is how do we make sure that this plays out well? And so that's actually the first question that\n",
            "[08:14.320 --> 08:19.760]  I would ask a powerful AGI system is. So you wouldn't ask your colleague, you wouldn't ask,\n",
            "[08:19.760 --> 08:24.320]  like Ilya, you would ask the AGI system. Oh, we've already had the conversation with Ilya,\n",
            "[08:24.320 --> 08:29.920]  right? And everyone here. And so you want as many perspectives and a piece of wisdom as you can\n",
            "[08:29.920 --> 08:34.560]  for answering this question. So I don't think you necessarily defer to whatever your powerful system\n",
            "[08:34.560 --> 08:40.800]  tells you, but you use it as one input to try to figure out what to do. But I guess fundamentally,\n",
            "[08:40.800 --> 08:44.400]  what it really comes down to is if you built something really powerful and you think about,\n",
            "[08:44.400 --> 08:48.880]  think about, for example, the creation of shortly after the creation of nuclear weapons, right? The\n",
            "[08:48.880 --> 08:53.600]  most important question in the world was what's the world order going to be like? How do we set\n",
            "[08:53.600 --> 08:59.120]  ourselves up in a place where we're going to be able to survive as a species? With AGI, I think\n",
            "[08:59.120 --> 09:02.800]  the question is slightly different, right? That there is a question of how do we make sure that\n",
            "[09:02.800 --> 09:07.200]  we don't get the negative effects, but there's also the positive side, right? You imagine that,\n",
            "[09:07.200 --> 09:12.240]  you know, like, like what won't AGI be like? Like what will it be capable of? And I think that one\n",
            "[09:12.240 --> 09:17.760]  of the core reasons that an AGI can be powerful and transformative is actually due to technological\n",
            "[09:17.760 --> 09:22.160]  development, right? If you have something that's capable, that's capable as a human, and that it's\n",
            "[09:22.160 --> 09:27.520]  much more scalable, that you absolutely want that thing to go read the whole scientific literature\n",
            "[09:27.520 --> 09:31.200]  and think about how to create cures for all the diseases, right? You want it to think about how to\n",
            "[09:31.200 --> 09:36.880]  go and build technologies to help us create material abundance and to figure out societal\n",
            "[09:36.880 --> 09:40.240]  problems that we have trouble with, like how we're supposed to clean up the environment. And,\n",
            "[09:40.240 --> 09:44.560]  you know, maybe you want this to go and invent a bunch of little robots that will go out and\n",
            "[09:44.560 --> 09:52.240]  be biodegradable and turn ocean debris into harmless molecules. And I think that that\n",
            "[09:53.040 --> 09:57.200]  positive side is something that I think people miss sometimes when thinking about what an AGI\n",
            "[09:57.200 --> 10:02.080]  will be like. And so I think that if you have a system that's capable of all of that, you absolutely\n",
            "[10:02.080 --> 10:08.160]  want its advice about how do I make sure that we're using your capabilities in a positive way\n",
            "[10:08.160 --> 10:13.760]  for humanity. So what do you think about that psychology that looks at all the different\n",
            "[10:13.760 --> 10:19.920]  possible trajectories of an AGI system, many of which, perhaps the majority of which are positive,\n",
            "[10:19.920 --> 10:24.320]  and nevertheless focuses on the negative trajectories? I mean, you get to interact with\n",
            "[10:24.320 --> 10:30.000]  folks, you get to think about this, maybe within yourself as well. You look at Sam Harris and so\n",
            "[10:30.000 --> 10:35.040]  on. It seems to be, sorry to put it this way, but almost more fun to think about the negative\n",
            "[10:35.600 --> 10:40.720]  possibilities. Whatever that's deep in our psychology, what do you think about that?\n",
            "[10:40.720 --> 10:47.280]  And how do we deal with it? Because we want AI to help us. So I think there's kind of two problems\n",
            "[10:48.240 --> 10:54.160]  entailed in that question. The first is more of the question of how can you even picture what a\n",
            "[10:54.160 --> 10:58.720]  world with a new technology will be like? Now imagine we're in 1950 and I'm trying to describe\n",
            "[10:58.720 --> 11:08.800]  Uber to someone. Apps and the internet. Yeah, I mean, that's going to be extremely complicated,\n",
            "[11:08.800 --> 11:14.960]  but it's imaginable. It's imaginable, right? And now imagine being in 1950 and predicting Uber,\n",
            "[11:14.960 --> 11:19.760]  right? And you need to describe the internet, you need to describe GPS, you need to describe\n",
            "[11:19.760 --> 11:25.360]  the fact that everyone's going to have this phone in their pocket. And so I think that just the\n",
            "[11:25.360 --> 11:30.000]  first truth is that it is hard to picture how a transformative technology will play out in the\n",
            "[11:30.000 --> 11:35.440]  world. We've seen that before with technologies that are far less transformative than AGI will be.\n",
            "[11:35.440 --> 11:41.120]  And so I think that one piece is that it's just even hard to imagine and to really put yourself\n",
            "[11:41.120 --> 11:47.840]  in a world where you can predict what that positive vision would be like. And I think the\n",
            "[11:47.840 --> 11:54.400]  second thing is that it is, I think it is always easier to support the negative side than the\n",
            "[11:54.400 --> 11:59.680]  positive side. It's always easier to destroy than create. And you know, less in a, in a,\n",
            "[11:59.680 --> 12:03.680]  in a physical sense and more just in a, in an intellectual sense, right? Because, you know,\n",
            "[12:03.680 --> 12:08.320]  I think that with creating something, you need to just get a bunch of things right and to destroy,\n",
            "[12:08.320 --> 12:12.560]  you just need to get one thing wrong. And so I think that what that means is that I think a lot\n",
            "[12:12.560 --> 12:18.560]  of people's thinking dead ends as soon as they see the negative story. But that being said,\n",
            "[12:18.560 --> 12:23.120]  I actually, actually have some hope, right? I think that, that the, that the positive vision\n",
            "[12:23.120 --> 12:28.400]  is something that I think can be, is something that we can, we can talk about. And I think that\n",
            "[12:28.400 --> 12:32.160]  just simply saying this fact of, yeah, like there's positive, there's negatives, everyone\n",
            "[12:32.160 --> 12:35.760]  likes to dwell on the negative. People actually respond well to that message and say, huh,\n",
            "[12:35.760 --> 12:38.960]  you're right. There's a part of this that we're not talking about, not thinking about.\n",
            "[12:38.960 --> 12:44.080]  And that's actually something that's, that's, that's, I think really been a key part of how\n",
            "[12:44.080 --> 12:49.200]  we think about AGI at OpenAI, right? You can kind of look at it as like, okay, like OpenAI talks\n",
            "[12:49.200 --> 12:54.320]  about the fact that there are risks and yet they're trying to build this system. Like how do you square\n",
            "[12:54.320 --> 13:00.080]  those, those two facts? So do you share the intuition that some people have, I mean, from Sam\n",
            "[13:00.080 --> 13:08.560]  Harris to even Elon Musk himself, that it's tricky as you develop AGI to keep it from slipping into\n",
            "[13:08.560 --> 13:15.920]  the existential threats into the negative. What's your intuition about how hard is it to keep AI\n",
            "[13:16.800 --> 13:21.520]  development on the positive track? What's your intuition there? To answer the question,\n",
            "[13:21.520 --> 13:26.240]  you can really look at how we structure OpenAI. So we really have three main arms. We have\n",
            "[13:26.240 --> 13:31.120]  capabilities, which is actually doing the technical work and pushing forward what these systems can do.\n",
            "[13:31.120 --> 13:36.880]  There's safety, which is working on technical mechanisms to ensure that the systems we build\n",
            "[13:36.880 --> 13:41.280]  are aligned with human values. And then there's policy, which is making sure that we have governance\n",
            "[13:41.280 --> 13:47.120]  mechanisms answering that question of, well, whose values. And so I think that the technical safety\n",
            "[13:47.120 --> 13:51.920]  one is the one that people kind of talk about the most, right? You talk about, like think about,\n",
            "[13:51.920 --> 13:56.720]  you know, all of the dystopic AI movies, a lot of that is about not having good technical safety\n",
            "[13:56.720 --> 14:01.360]  in place. And what we've been finding is that, you know, I think that actually a lot of people\n",
            "[14:01.360 --> 14:03.840]  look at the technical safety problem and think it's just intractable.\n",
            "[14:03.840 --> 14:08.160]  Right. This question of what do humans want? How am I supposed to write that down? Can I even write\n",
            "[14:08.160 --> 14:15.600]  down what I want? No way. And then they stop there. But the thing is we've already built systems that\n",
            "[14:15.600 --> 14:21.600]  are able to learn things that humans can't specify. You know, even the rules for how to recognize if\n",
            "[14:21.600 --> 14:25.760]  there's a cat or a dog in an image, it turns out it's intractable to write that down. And yet we're\n",
            "[14:25.760 --> 14:30.640]  able to learn it. And that what we're seeing with systems we build at OpenAI, and they're still in\n",
            "[14:30.640 --> 14:35.600]  early proof of concept stage, is that you are able to learn human preferences. You're able to learn\n",
            "[14:35.600 --> 14:40.640]  what humans want from data. And so that's kind of the core focus for our technical safety team. And\n",
            "[14:40.640 --> 14:45.200]  I think that there actually we've had some pretty encouraging updates in terms of what we've been\n",
            "[14:45.200 --> 14:51.040]  able to make work. So you have an intuition and a hope that from data, you know, looking at the\n",
            "[14:51.040 --> 14:57.600]  value alignment problem from data, we can build systems that align with the collective better\n",
            "[14:57.600 --> 15:04.000]  angels of our nature. So align with the ethics and the morals of human beings. To even say this in a\n",
            "[15:04.000 --> 15:08.640]  different way. I mean, think about how do we align humans, right? Think about like a human baby can\n",
            "[15:08.640 --> 15:13.440]  grow up to be an evil person or a great person. And a lot of that is from learning from data,\n",
            "[15:13.440 --> 15:17.760]  right? That you have some feedback as a child is growing up, they get to see positive examples. And\n",
            "[15:17.760 --> 15:25.120]  so I think that, that just like the only example we have of a general intelligence that is able to\n",
            "[15:25.120 --> 15:31.280]  learn from data to align with human values and to learn values. I think we shouldn't be surprised\n",
            "[15:31.280 --> 15:36.400]  that we can do the same sorts of techniques or whether the same sort of techniques end up being\n",
            "[15:36.400 --> 15:41.840]  how we solve value alignment for AGI's. So let's go even higher. I don't know if you've read the\n",
            "[15:41.840 --> 15:48.720]  book Sapiens, but there's an idea that, you know, that as a collective, as us human beings who kind\n",
            "[15:48.720 --> 15:56.640]  of develop together and ideas that we hold, there's no in that context, objective truth. We just kind\n",
            "[15:56.640 --> 16:01.840]  of all agree to certain ideas and hold them as a collective. Did you have a sense that there is in\n",
            "[16:01.840 --> 16:06.480]  the world of good and evil, do you have a sense that to the first approximation, there are some\n",
            "[16:06.480 --> 16:13.840]  things that are good and that you could teach systems to behave to be good? So I think that\n",
            "[16:13.840 --> 16:19.360]  this actually blends into our third team, right? Which is the policy team. And this is the one,\n",
            "[16:19.360 --> 16:24.000]  the aspect that I think people really talk about way less than they should, right? Because imagine\n",
            "[16:24.000 --> 16:28.160]  that we build super powerful systems that we've managed to figure out all the mechanisms for these\n",
            "[16:28.160 --> 16:33.680]  things to do whatever the operator wants. The most important question becomes who's the operator,\n",
            "[16:33.680 --> 16:39.040]  what do they want and how is that going to affect everyone else, right? And I think that this\n",
            "[16:39.040 --> 16:44.320]  question of what is good, what are those values? I mean, I think you don't even have to go to those,\n",
            "[16:44.320 --> 16:49.040]  those, those very grand existential places to start to realize how hard this problem is. You\n",
            "[16:49.040 --> 16:54.320]  just look at different countries and cultures across the world and that there's, there's a very\n",
            "[16:54.320 --> 17:00.160]  different conception of how the world works and you know, what, what, what kinds of, of ways that\n",
            "[17:00.160 --> 17:06.320]  society wants to operate. And so I think that, that, that the really core question is, is, is\n",
            "[17:06.320 --> 17:10.640]  actually very concrete. And I think it's not a question that we have ready answers to, right?\n",
            "[17:10.640 --> 17:15.920]  It's how do you have a world where all of the different countries that we have, United States,\n",
            "[17:15.920 --> 17:21.360]  China, Russia, and you know, the, the, the hundreds of other countries out there are able to\n",
            "[17:21.360 --> 17:28.000]  continue to not just operate in the way that they, that they, that they see fit, but in, in a, that,\n",
            "[17:28.000 --> 17:35.760]  that the world that emerges in these, where you have these very powerful systems operate in a\n",
            "[17:35.760 --> 17:40.240]  way that's operating alongside humans ends up being something that empowers humans more,\n",
            "[17:40.240 --> 17:46.240]  that makes like human existence be a more meaningful thing and that people are happier\n",
            "[17:46.240 --> 17:50.960]  and wealthier and able to live more fulfilling lives. It's not an obvious thing for how to design\n",
            "[17:50.960 --> 17:56.720]  that world once you have that very powerful system. So if we take a little step back and we're\n",
            "[17:56.720 --> 18:02.640]  having like a fascinating conversation and OpenAI is in many ways a tech leader in the world. And\n",
            "[18:02.640 --> 18:07.200]  we're thinking about these big existential questions, which is fascinating, really important.\n",
            "[18:07.200 --> 18:12.320]  I think you're a leader in that space and that's a really important space of just thinking how AI\n",
            "[18:12.320 --> 18:18.000]  affects society in a big picture view. So Oscar Wilde said, we're all in the gutter, but some of\n",
            "[18:18.000 --> 18:24.080]  us are looking at the stars. And I think OpenAI has a charter that looks to the stars, I would say,\n",
            "[18:24.720 --> 18:28.400]  to create intelligence, to create general intelligence, make it beneficial, safe,\n",
            "[18:28.400 --> 18:35.840]  and collaborative. So can you tell me how that came about, how a mission like that and the path\n",
            "[18:36.480 --> 18:39.280]  to creating a mission like that at OpenAI was founded?\n",
            "[18:39.280 --> 18:45.040]  Yeah. So I think that in some ways it really boils down to taking a look at the landscape.\n",
            "[18:45.040 --> 18:50.080]  So if you think about the history of AI, that basically for the past 60 or 70 years,\n",
            "[18:50.080 --> 18:54.720]  people have thought about this goal of what could happen if you could automate human\n",
            "[18:54.720 --> 18:59.920]  intellectual labor. Imagine you could build a computer system that could do that. What becomes\n",
            "[18:59.920 --> 19:04.640]  possible? We have a lot of sci-fi that tells stories of various dystopias. And increasingly,\n",
            "[19:04.640 --> 19:08.400]  you have movies like Her that tell you a little bit about maybe more of a little bit utopic vision.\n",
            "[19:09.520 --> 19:16.400]  You think about the impacts that we've seen from being able to have bicycles for our minds\n",
            "[19:16.400 --> 19:23.520]  and computers. And I think that the impact of computers and the internet has just far outstripped\n",
            "[19:23.520 --> 19:28.400]  what anyone really could have predicted. And so I think that it's very clear that if you can build\n",
            "[19:28.400 --> 19:32.720]  an AGI, it will be the most transformative technology that humans will ever create.\n",
            "[19:34.560 --> 19:39.760]  And so what it boils down to then is a question of, well, is there a path? Is there hope? Is there\n",
            "[19:39.760 --> 19:45.120]  a way to build such a system? And I think that for 60 or 70 years that people got excited and\n",
            "[19:46.160 --> 19:51.600]  that ended up not being able to deliver on the hopes that people had pinned on them.\n",
            "[19:51.600 --> 19:59.040]  And I think that then after two winters of AI development, that people I think almost\n",
            "[19:59.040 --> 20:04.800]  stopped daring to dream. Really talking about AGI or thinking about AGI became almost this taboo\n",
            "[20:04.800 --> 20:10.080]  in the community. But I actually think that people took the wrong lesson from AI history.\n",
            "[20:10.080 --> 20:15.440]  And if you look back, starting in 1959 is when the Perceptron was released. And this is basically\n",
            "[20:15.440 --> 20:20.000]  one of the earliest neural networks. It was released to what was perceived as this massive\n",
            "[20:20.000 --> 20:25.600]  overhype. So in the New York Times in 1959, you have this article saying that the Perceptron\n",
            "[20:26.320 --> 20:31.440]  will one day recognize people, call out their names, instantly translate speech between languages.\n",
            "[20:31.440 --> 20:36.480]  And people at the time looked at this and said, your system can't do any of that. And basically\n",
            "[20:36.480 --> 20:41.040]  spent 10 years trying to discredit the whole Perceptron direction and succeeded. And all the\n",
            "[20:41.040 --> 20:46.320]  funding dried up. And people kind of went in other directions. And in the 80s, there was\n",
            "[20:46.320 --> 20:50.560]  resurgence. And I'd always heard that the resurgence in the 80s was due to the invention\n",
            "[20:50.560 --> 20:55.200]  of backpropagation and these algorithms that got people excited. But actually, the causality was\n",
            "[20:55.200 --> 20:59.440]  due to people building larger computers. You can find these articles from the 80s saying that the\n",
            "[20:59.440 --> 21:04.000]  democratization of computing power suddenly meant that you could run these larger neural networks.\n",
            "[21:04.000 --> 21:08.080]  And then people started to do all these amazing things. The backpropagation algorithm was invented.\n",
            "[21:08.080 --> 21:12.160]  And the neural nets people were running were these tiny little 20 neuron neural nets.\n",
            "[21:12.160 --> 21:16.080]  Right? Like, what are you supposed to learn with 20 neurons? And so of course,\n",
            "[21:16.720 --> 21:21.840]  they weren't able to get great results. And it really wasn't until 2012 that this approach,\n",
            "[21:21.840 --> 21:26.800]  that's almost the most simple, natural approach that people had come up with in the 50s.\n",
            "[21:27.360 --> 21:31.760]  Right? In some ways, even in the 40s, before there were computers with the Pitts-McCullough neuron,\n",
            "[21:33.120 --> 21:38.480]  suddenly this became the best way of solving problems. Right? And I think there are three core\n",
            "[21:38.480 --> 21:44.000]  properties that deep learning has that I think are very worth paying attention to.\n",
            "[21:44.000 --> 21:49.920]  The first is generality. We have a very small number of deep learning tools, SGD, deep neural\n",
            "[21:49.920 --> 21:56.560]  net, maybe some RL, and it solves this huge variety of problems. Speech recognition, machine\n",
            "[21:56.560 --> 22:02.000]  translation, game playing, all of these problems, small set of tools. So there's the generality.\n",
            "[22:02.800 --> 22:06.240]  There's a second piece, which is the competence. You want to solve any of those problems?\n",
            "[22:06.240 --> 22:11.040]  Throw out 40 years worth of normal computer vision research, replace it with a deep neural net,\n",
            "[22:11.040 --> 22:16.080]  it's going to work better. And there's a third piece, which is the scalability. Right? That\n",
            "[22:16.080 --> 22:20.160]  one thing that has been shown time and time again is that you, if you have a larger neural network,\n",
            "[22:21.040 --> 22:27.040]  throw more compute, more data at it, it will work better. Those three properties together feel like\n",
            "[22:27.040 --> 22:32.560]  essential parts of building a general intelligence. Now, it doesn't just mean that if we scale up what\n",
            "[22:32.560 --> 22:37.280]  we have, that we will have an AGI. There are clearly missing pieces. There are missing ideas.\n",
            "[22:37.280 --> 22:44.160]  We need to have answers for reasoning. But I think that the core here is that for the first time,\n",
            "[22:44.160 --> 22:48.960]  it feels that we have a paradigm that gives us hope that general intelligence can be achievable.\n",
            "[22:49.920 --> 22:55.120]  And so as soon as you believe that, everything else comes into focus. If you imagine that you\n",
            "[22:55.120 --> 23:00.800]  may be able to, and that the timeline I think remains uncertain, but I think that certainly\n",
            "[23:00.800 --> 23:05.440]  within our lifetimes and possibly within a much shorter period of time than people would expect,\n",
            "[23:06.000 --> 23:10.640]  if you can really build the most transformative technology that will ever exist, you stop thinking\n",
            "[23:10.640 --> 23:14.640]  about yourself so much, right? And you start thinking about just like, how do you have a world\n",
            "[23:14.640 --> 23:18.240]  where this goes well? And that you need to think about the practicalities of how do you build an\n",
            "[23:18.240 --> 23:23.040]  organization and get together a bunch of people and resources and to make sure that people feel\n",
            "[23:23.040 --> 23:29.840]  motivated and ready to do it. But I think that then you start thinking about, well, what if we\n",
            "[23:29.840 --> 23:34.560]  succeed? And how do we make sure that when we succeed, that the world is actually the place that\n",
            "[23:34.560 --> 23:40.720]  we want ourselves to exist in? And almost in the Rawlsian-Bale sense of the word. And so that's\n",
            "[23:40.720 --> 23:47.680]  kind of the broader landscape. And OpenAI was really formed in 2015 with that high-level picture\n",
            "[23:47.680 --> 23:54.480]  of AGI might be possible sooner than people think, and that we need to try to do our best to make\n",
            "[23:54.480 --> 23:58.720]  sure it's going to go well. And then we spent the next couple of years really trying to figure out,\n",
            "[23:58.720 --> 24:04.960]  what does that mean? How do we do it? And I think that typically with a company, you start out very\n",
            "[24:04.960 --> 24:08.560]  small, see you in a co-founder and you build a product, you get some users, you get product\n",
            "[24:08.560 --> 24:14.720]  market fit, then at some point you raise some money, you hire people, you scale, and then down\n",
            "[24:14.720 --> 24:19.040]  the road, then the big companies realize you exist and try to kill you. And for OpenAI,\n",
            "[24:19.040 --> 24:21.200]  it was basically everything in exactly the opposite order.\n",
            "[24:21.200 --> 24:28.880]  Let me just pause for a second. You said a lot of things. And let me just admire the jarring\n",
            "[24:28.880 --> 24:35.360]  aspect of what OpenAI stands for, which is daring to dream. I mean, you said it's pretty powerful.\n",
            "[24:35.360 --> 24:42.560]  You caught me off guard because I think that's very true. The step of just daring to dream about\n",
            "[24:42.560 --> 24:47.920]  the possibilities of creating intelligence in a positive, in a safe way, but just even creating\n",
            "[24:47.920 --> 24:56.720]  intelligence is a much needed refreshing catalyst for the AI community. So that's the starting\n",
            "[24:56.720 --> 24:59.680]  point. Okay. So then formation of OpenAI.\n",
            "[24:59.680 --> 25:06.000]  I would just say that when we were starting OpenAI, the first question that we had is,\n",
            "[25:06.000 --> 25:09.760]  is it too late to start a lab with a bunch of the best people?\n",
            "[25:09.760 --> 25:10.260]  Wow.\n",
            "[25:10.260 --> 25:12.760]  Is that even possible? That was an actual question?\n",
            "[25:12.760 --> 25:18.840]  That was the core question of, we had this dinner in July of 2015, and that was really what we\n",
            "[25:18.840 --> 25:25.400]  spent the whole time talking about. And because you think about where AI was, is that it had\n",
            "[25:25.400 --> 25:31.080]  transitioned from being an academic pursuit to an industrial pursuit. And so a lot of the best\n",
            "[25:31.080 --> 25:36.280]  people were in these big research labs and that we wanted to start our own one that no matter how\n",
            "[25:36.280 --> 25:41.560]  much resources we could accumulate would be pale in comparison to the big tech companies.\n",
            "[25:41.560 --> 25:46.040]  We knew that. And there was a question of, are we going to be actually able to get this thing off\n",
            "[25:46.040 --> 25:50.040]  the ground? You need critical mass. You can't just do you and a co-founder build a product,\n",
            "[25:50.040 --> 25:56.200]  right? You really need to have a group of five to 10 people. And we kind of concluded it wasn't\n",
            "[25:56.200 --> 25:58.600]  obviously impossible. So it seemed worth trying.\n",
            "[25:58.600 --> 26:02.920]  Well, you're also a dreamer, so who knows, right?\n",
            "[26:02.920 --> 26:03.420]  That's right.\n",
            "[26:03.420 --> 26:11.720]  Okay. So speaking of that, competing with the big players, let's talk about some of the tricky\n",
            "[26:11.720 --> 26:17.760]  things as you think through this process of growing, of seeing how you can develop these\n",
            "[26:17.760 --> 26:27.600]  systems at scale that competes. So you recently formed OpenAI LP, a new cap profit company that\n",
            "[26:27.600 --> 26:33.920]  now carries the name OpenAI. So OpenAI is now this official company. The original nonprofit\n",
            "[26:33.920 --> 26:40.000]  company still exists and carries the OpenAI nonprofit name. So can you explain what this\n",
            "[26:40.000 --> 26:47.920]  company is, what the purpose of this creation is, and how did you arrive at the decision to create it?\n",
            "[26:47.920 --> 26:54.880]  OpenAI, the whole entity, and OpenAI LP as a vehicle, is trying to accomplish the mission of\n",
            "[26:54.880 --> 26:58.640]  ensuring that artificial general intelligence benefits everyone. And the main way that we're\n",
            "[26:58.640 --> 27:02.800]  trying to do that is by actually trying to build general intelligence ourselves and make sure the\n",
            "[27:02.800 --> 27:07.680]  benefits are distributed to the world. That's the primary way. We're also fine if someone else does\n",
            "[27:07.680 --> 27:12.560]  this, right? It doesn't have to be us. If someone else is going to build an AGI and make sure that\n",
            "[27:12.560 --> 27:19.280]  the benefits don't get locked up in one company or with one set of people, we're actually fine with\n",
            "[27:19.280 --> 27:27.360]  that. And so those ideas are baked into our charter, which is the foundational document\n",
            "[27:27.360 --> 27:34.160]  that describes our values and how we operate. It's also really baked into the structure of\n",
            "[27:34.160 --> 27:41.440]  OpenAI LP. And so the way that we set up OpenAI LP is that in the case where we succeed, if we\n",
            "[27:41.440 --> 27:47.680]  actually build what we're trying to build, then investors are able to get a return. But that\n",
            "[27:47.680 --> 27:52.400]  return is something that is capped. And so if you think of AGI in terms of the value that you could\n",
            "[27:52.400 --> 27:56.400]  really create, you're talking about the most transformative technology ever created, is going\n",
            "[27:56.400 --> 28:03.040]  to create orders of magnitude more value than any existing company, and that all of that value will\n",
            "[28:03.040 --> 28:10.160]  be owned by the world, like legally titled to the nonprofit to fulfill that mission. And so that's\n",
            "[28:10.160 --> 28:17.120]  the structure. So the mission is a powerful one, and it's one that I think most people are\n",
            "[28:17.120 --> 28:24.000]  going to agree with. It's how we would hope AI progresses. And so how do you tie yourself to that\n",
            "[28:24.000 --> 28:32.160]  mission? How do you make sure you do not deviate from that mission, that other incentives that are\n",
            "[28:32.160 --> 28:38.720]  profit driven don't interfere with the mission? So this was actually a really core question for us\n",
            "[28:38.720 --> 28:43.520]  for the past couple of years. Because I'd say that the way that our history went was that for the\n",
            "[28:43.520 --> 28:48.160]  first year we were getting off the ground. We had this high level picture, but we didn't know\n",
            "[28:48.160 --> 28:53.920]  exactly how we wanted to accomplish it. And really two years ago is when we first started realizing\n",
            "[28:53.920 --> 28:59.200]  in order to build AGI, we're just going to need to raise way more money than we can as a nonprofit.\n",
            "[28:59.200 --> 29:05.360]  And we're talking many billions of dollars. And so the first question is, how are you supposed to\n",
            "[29:05.360 --> 29:09.680]  do that and stay true to this mission? And we looked at every legal structure out there and\n",
            "[29:09.680 --> 29:13.200]  concluded none of them were quite right for what we wanted to do. And I guess it shouldn't be too\n",
            "[29:13.200 --> 29:16.560]  surprising if you're going to do some crazy unprecedented technology that you're going to have\n",
            "[29:16.560 --> 29:22.400]  to come with some crazy unprecedented structure to do it in. And a lot of our conversation was\n",
            "[29:22.960 --> 29:28.160]  with people at OpenAI, the people who really joined because they believe so much in this mission,\n",
            "[29:28.160 --> 29:34.080]  and thinking about how do we actually raise the resources to do it and also stay true to what we\n",
            "[29:34.080 --> 29:38.400]  stand for. And the place you've got to start is to really align on what is it that we stand for?\n",
            "[29:38.400 --> 29:41.920]  Right? What are those values? What's really important to us? And so I'd say that we spent\n",
            "[29:41.920 --> 29:47.760]  about a year really compiling the OpenAI charter and that determines, and if you even look at the\n",
            "[29:47.760 --> 29:51.680]  first line item in there, it says that, look, we expect we're going to have to marshal huge\n",
            "[29:51.680 --> 29:55.840]  amounts of resources, but we're going to make sure that we minimize conflict of interest with the\n",
            "[29:55.840 --> 30:01.920]  mission. And that kind of aligning on all of those pieces was the most important step towards\n",
            "[30:01.920 --> 30:08.480]  figuring out how do we structure a company that can actually raise the resources to do what we\n",
            "[30:08.480 --> 30:15.520]  need to do. I imagine OpenAI, the decision to create OpenAI LP was a really difficult one.\n",
            "[30:15.520 --> 30:20.800]  And there was a lot of discussions, as you mentioned, for a year, and there was different\n",
            "[30:20.800 --> 30:27.840]  ideas, perhaps detractors within OpenAI, sort of different paths that you could have taken.\n",
            "[30:27.840 --> 30:32.160]  What were those concerns? What were the different paths considered? What was that process of making\n",
            "[30:32.160 --> 30:37.440]  that decision like? Yep. But so if you look actually at the OpenAI charter, that there's almost\n",
            "[30:37.440 --> 30:44.000]  two paths embedded within it. There is, we are primarily trying to build AGI ourselves, but we're\n",
            "[30:44.000 --> 30:48.720]  also okay if someone else does it. And this is a weird thing for a company. It's really interesting,\n",
            "[30:48.720 --> 30:54.800]  actually. Yeah. There is an element of competition that you do want to be the one that does it,\n",
            "[30:54.800 --> 30:58.160]  but at the same time, you're okay if somebody else doesn't. And we'll talk about that a little bit,\n",
            "[30:58.160 --> 31:01.920]  that trade-off, that dance that's really interesting. And I think this was the core\n",
            "[31:01.920 --> 31:07.520]  tension as we were designing OpenAI LP and really the OpenAI strategy is how do you make sure that\n",
            "[31:07.520 --> 31:13.120]  both you have a shot at being a primary actor, which really requires building an organization,\n",
            "[31:13.760 --> 31:18.480]  raising massive resources, and really having the will to go and execute on some really,\n",
            "[31:18.480 --> 31:22.800]  really hard vision, right? You need to really sign up for a long period to go and take a\n",
            "[31:22.800 --> 31:29.440]  lot of pain and a lot of risk. And to do that, normally you just import the startup mindset,\n",
            "[31:29.440 --> 31:33.040]  right? And that you think about, okay, how do we out-execute everyone? You have this very\n",
            "[31:33.040 --> 31:37.680]  competitive angle. But you also have the second angle of saying that, well, the true mission isn't\n",
            "[31:37.680 --> 31:44.400]  for OpenAI to build AGI. The true mission is for AGI to go well for humanity. And so, how do you\n",
            "[31:44.400 --> 31:49.840]  take all of those first actions and make sure you don't close the door on outcomes that would\n",
            "[31:49.840 --> 31:54.480]  actually be positive and fulfill the mission? And so, I think it's a very delicate balance,\n",
            "[31:54.480 --> 31:59.360]  right? And I think that going 100% one direction or the other is clearly not the correct answer.\n",
            "[31:59.360 --> 32:03.440]  And so, I think that even in terms of just how we talk about OpenAI and think about it,\n",
            "[32:03.440 --> 32:08.400]  there's just like one thing that's always in the back of my mind is to make sure that we're not\n",
            "[32:08.400 --> 32:13.600]  just saying OpenAI's goal is to build AGI, right? That it's actually much broader than that, right?\n",
            "[32:13.600 --> 32:19.360]  That first of all, it's not just AGI, it's safe AGI that's very important. And so,\n",
            "[32:19.360 --> 32:23.520]  it's very important. But secondly, our goal isn't to be the ones to build it. Our goal is to make\n",
            "[32:23.520 --> 32:27.280]  sure it goes well for the world. And so, I think that figuring out how do you balance all of those\n",
            "[32:27.920 --> 32:35.680]  and to get people to really come to the table and compile a single document that encompasses all of\n",
            "[32:35.680 --> 32:43.440]  that wasn't trivial. So, part of the challenge here is your mission is, I would say, beautiful,\n",
            "[32:43.440 --> 32:48.240]  empowering, and a beacon of hope for people in the research community and just people thinking\n",
            "[32:48.240 --> 32:55.840]  about AI. So, your decisions are scrutinized more than, I think, a regular profit-driven company.\n",
            "[32:55.840 --> 33:00.160]  Do you feel the burden of this in the creation of the charter and just in the way you operate?\n",
            "[33:00.160 --> 33:00.660]  Yes.\n",
            "[33:02.960 --> 33:10.400]  So, why do you lean into the burden by creating such a charter? Why not keep it quiet?\n",
            "[33:10.400 --> 33:15.440]  I mean, it just boils down to the mission, right? Like, I'm here and everyone else is here because\n",
            "[33:15.440 --> 33:21.040]  we think this is the most important mission, right? Dare to dream. All right. So, do you think you can\n",
            "[33:21.040 --> 33:28.320]  be good for the world or create an AGI system that's good when you're a for-profit company?\n",
            "[33:28.320 --> 33:36.960]  From my perspective, I don't understand why profit interferes with positive impact on society.\n",
            "[33:37.600 --> 33:44.320]  I don't understand why Google that makes most of its money from ads can't also do good for the\n",
            "[33:44.320 --> 33:49.920]  world or other companies, Facebook, anything. I don't understand why those have to interfere.\n",
            "[33:51.920 --> 33:58.160]  Profit isn't the thing, in my view, that affects the impact of a company. What affects the impact\n",
            "[33:58.160 --> 34:05.360]  of the company is the charter, is the culture, is the people inside. And profit is the thing that\n",
            "[34:05.360 --> 34:10.960]  just fuels those people. What are your views there? Yeah. So, I think that's a really good question.\n",
            "[34:10.960 --> 34:15.760]  There's some real long-standing debates in human society that are wrapped up in it.\n",
            "[34:16.320 --> 34:20.960]  The way that I think about it is just think about what are the most impactful non-profits in the\n",
            "[34:20.960 --> 34:28.240]  world? What are the most impactful for-profits in the world? Right. It's much easier to list\n",
            "[34:28.240 --> 34:33.840]  the for-profits. That's right. And I think that there's some real truth here that the system that\n",
            "[34:33.840 --> 34:40.240]  we set up, the system for kind of how today's world is organized is one that really allows for\n",
            "[34:40.240 --> 34:50.240]  huge impact. And part of that is that for-profits are self-sustaining and able to build on their\n",
            "[34:50.240 --> 34:55.840]  own momentum. And I think that's a really powerful thing. It's something that when it turns out that\n",
            "[34:55.840 --> 35:00.000]  we haven't set the guardrails correctly, causes problems. Think about logging companies that go\n",
            "[35:00.000 --> 35:05.920]  into the rainforest. That's really bad. We don't want that. And it's actually really interesting\n",
            "[35:05.920 --> 35:11.280]  to me that this question of how do you get positive benefits out of a for-profit company,\n",
            "[35:11.280 --> 35:14.560]  it's actually very similar to how do you get positive benefits out of an AGI?\n",
            "[35:15.920 --> 35:21.120]  You have this very powerful system. It's more powerful than any human and is kind of autonomous\n",
            "[35:21.120 --> 35:25.440]  in some ways. It's superhuman in a lot of axes. And somehow you have to set the guardrails to\n",
            "[35:25.440 --> 35:30.720]  get good things to happen. But when you do, the benefits are massive. And so I think that\n",
            "[35:30.720 --> 35:36.640]  when I think about nonprofit versus for-profit, I think just not enough happens in nonprofits.\n",
            "[35:36.640 --> 35:42.240]  They're very pure, but it's just hard to do things there. And for-profits in some ways,\n",
            "[35:42.240 --> 35:48.880]  too much happens. But if shaped in the right way, it can actually be very positive. And so with\n",
            "[35:48.880 --> 35:53.920]  OpenAI LP, we're picking a road in between. Now, the thing that I think is really important to\n",
            "[35:53.920 --> 35:59.680]  recognize is that the way that we think about OpenAI LP is that in the world where AGI actually\n",
            "[35:59.680 --> 36:03.600]  happens, in a world where we are successful, we build the most transformative technology ever,\n",
            "[36:03.600 --> 36:05.840]  the amount of value we're going to create will be astronomical.\n",
            "[36:07.520 --> 36:14.880]  And so then in that case, the cap that we have will be a small fraction of the value we create.\n",
            "[36:15.680 --> 36:19.200]  And the amount of value that goes back to investors and employees looks pretty similar\n",
            "[36:19.200 --> 36:25.200]  to what would happen in a pretty successful startup. And that's really the case that we're\n",
            "[36:25.200 --> 36:30.880]  optimizing for, that we're thinking about in the success case, making sure that the value we create\n",
            "[36:30.880 --> 36:36.320]  doesn't get locked up. And I expect that in other for-profit companies that it's possible to do\n",
            "[36:36.320 --> 36:41.360]  something like that. I think it's not obvious how to do it. And I think that as a for-profit company,\n",
            "[36:41.360 --> 36:44.880]  you have a lot of fiduciary duty to your shareholders and that there are certain\n",
            "[36:44.880 --> 36:49.360]  decisions that you just cannot make. In our structure, we've set it up so that\n",
            "[36:49.360 --> 36:54.640]  we have a fiduciary duty to the charter, that we always get to make the decision that is right for\n",
            "[36:54.640 --> 37:01.440]  the charter, even if it comes at the expense of our own stakeholders. And so I think that\n",
            "[37:01.440 --> 37:05.360]  when I think about what's really important, it's not really about nonprofit versus for-profit.\n",
            "[37:05.360 --> 37:11.360]  It's really a question of if you build AGI and humanity is now in this new age,\n",
            "[37:12.160 --> 37:17.280]  who benefits, whose lives are better? And I think that what's really important is to have an answer\n",
            "[37:17.280 --> 37:23.680]  that is everyone. Yeah, which is one of the core aspects of the charter. So one concern people have,\n",
            "[37:24.400 --> 37:33.120]  not just with OpenAI, but with Google, Facebook, Amazon, anybody really that's creating impact at\n",
            "[37:33.120 --> 37:39.360]  scale is how do we avoid, as your charter says, avoid enabling the use of AI or AGI to\n",
            "[37:39.360 --> 37:46.400]  unduly concentrate power? Why would not a company like OpenAI keep all the power of an AGI system to\n",
            "[37:46.400 --> 37:55.200]  itself? The charter. The charter. So how does the charter actionalize itself in a day to day?\n",
            "[37:55.760 --> 38:01.040]  So I think that first to zoom out, that the way that we structure the company is so that the\n",
            "[38:01.040 --> 38:06.320]  power for sort of dictating the actions that OpenAI takes ultimately rests with the board.\n",
            "[38:06.320 --> 38:11.360]  The board of the nonprofit. And the board is set up in certain ways with certain restrictions that\n",
            "[38:11.360 --> 38:17.120]  you can read about in the OpenAI LP blog post. But effectively the board is the governing body for\n",
            "[38:17.120 --> 38:25.520]  OpenAI LP. And the board has a duty to fulfill the mission of the nonprofit. And so that's kind of\n",
            "[38:25.520 --> 38:30.560]  how we tie, how we thread all these things together. Now there's a question of, so day to day,\n",
            "[38:30.560 --> 38:34.320]  how do people, the individuals who in some ways are the most empowered,\n",
            "[38:34.320 --> 38:38.800]  individuals who in some ways are the most empowered ones, right? Now the board sort of gets to call\n",
            "[38:38.800 --> 38:43.520]  the shots at the high level, but the people who are actually executing are the employees,\n",
            "[38:43.520 --> 38:47.920]  right? The people here on a day to day basis who have the keys to the technical kingdom.\n",
            "[38:49.440 --> 38:54.560]  And there, I think that the answer looks a lot like, well, how does any company's values get\n",
            "[38:54.560 --> 38:58.640]  actualized, right? And I think that a lot of that comes down to the unique people who are here\n",
            "[38:58.640 --> 39:03.920]  because they really believe in that mission and they believe in the charter and that they\n",
            "[39:03.920 --> 39:09.280]  are willing to take actions that maybe are worse for them, but are better for the charter. And\n",
            "[39:09.280 --> 39:13.440]  that's something that's really baked into the culture. And honestly, I think it's, you know,\n",
            "[39:13.440 --> 39:18.080]  I think that that's one of the things that we really have to work to preserve as time goes on.\n",
            "[39:18.080 --> 39:22.560]  And that's a really important part of how we think about hiring people and bringing people into\n",
            "[39:22.560 --> 39:30.160]  OpenAI. So there's people here, there's people here who could speak up and say, like, hold on a\n",
            "[39:30.160 --> 39:35.600]  second. This is totally against what we stand for, culture wise. Yeah, yeah, for sure. I mean,\n",
            "[39:35.600 --> 39:41.120]  I think that we actually have, I think that's like a pretty important part of how we operate and how\n",
            "[39:41.120 --> 39:46.000]  we have, even again, with designing the charter and designing OpenAI LP in the first place,\n",
            "[39:46.560 --> 39:50.880]  that there has been a lot of conversation with employees here. And a lot of times where\n",
            "[39:50.880 --> 39:54.400]  employees said, wait a second, this seems like it's going in the wrong direction and let's talk\n",
            "[39:54.400 --> 39:58.800]  about it. And so I think one thing that's, I think a really, and you know, here's actually one thing\n",
            "[39:58.800 --> 40:04.240]  that I think is very unique about us as a small company is that if you're at a massive tech giant,\n",
            "[40:04.240 --> 40:08.960]  that's a little bit hard for someone who's aligned employee to go and talk to the CEO and say,\n",
            "[40:08.960 --> 40:13.840]  I think that we're doing this wrong. And you know, you'll get companies like Google that have had\n",
            "[40:13.840 --> 40:19.680]  some collective action from employees to make ethical change around things like Maven. And so\n",
            "[40:19.680 --> 40:23.840]  maybe there are mechanisms that other companies that work, but here, super easy for anyone to\n",
            "[40:23.840 --> 40:27.120]  pull me aside, to pull Sam aside, to pull Ilya aside and people do it all the time.\n",
            "[40:27.120 --> 40:32.080]  And one of the interesting things in the charter is this idea that it'd be great if you could try\n",
            "[40:32.080 --> 40:38.800]  to describe or untangle switching from competition to collaboration and late stage AGI development.\n",
            "[40:38.800 --> 40:42.640]  It's really interesting this dance between competition and collaboration. How do you think\n",
            "[40:42.640 --> 40:47.200]  about that? Yeah. Assuming that you can actually do the technical side of AGI development. I think\n",
            "[40:47.200 --> 40:50.800]  there's going to be two key problems with figuring out how do you actually deploy it and make it go\n",
            "[40:50.800 --> 40:57.200]  well. The first one of these is the run up to building the first AGI. You look at how self\n",
            "[40:57.200 --> 41:01.360]  driving cars are being developed and it's a competitive race. And the thing that always\n",
            "[41:01.360 --> 41:05.360]  happens in competitive race is that you have huge amounts of pressure to get rid of safety.\n",
            "[41:06.640 --> 41:10.640]  And so that's one thing we're very concerned about, right? Is that people, multiple teams\n",
            "[41:10.640 --> 41:17.760]  figuring out we can actually get there, but you know, if we took the slower path that is more\n",
            "[41:17.760 --> 41:23.360]  guaranteed to be safe, we will lose. And so we're going to take the fast path. And so the more that\n",
            "[41:23.360 --> 41:28.960]  we can both ourselves be in a position where we don't generate that competitive race, where we say,\n",
            "[41:28.960 --> 41:33.200]  if the race is being run and that, you know, someone else's is further ahead than we are,\n",
            "[41:33.200 --> 41:37.600]  we're not going to try to leapfrog. We're going to actually work with them, right? We will help\n",
            "[41:37.600 --> 41:42.960]  them succeed as long as what they're trying to do is to fulfill our mission. Then we're good. We\n",
            "[41:42.960 --> 41:46.960]  don't have to build AGI ourselves. And I think that's a really important commitment from us,\n",
            "[41:46.960 --> 41:51.440]  but it can't just be unilateral, right? I think that it's really important that other players who\n",
            "[41:51.440 --> 41:56.320]  are serious about building AGI make similar commitments, right? And I think that, you know,\n",
            "[41:56.320 --> 41:59.920]  again, to the extent that everyone believes that AGI should be something to benefit everyone,\n",
            "[41:59.920 --> 42:03.440]  then it actually really shouldn't matter which company builds it. And we should all be concerned\n",
            "[42:03.440 --> 42:06.880]  about the case where we just race so hard to get there that something goes wrong.\n",
            "[42:07.600 --> 42:14.080]  So what role do you think government, our favorite entity, has in setting policy and rules about\n",
            "[42:14.080 --> 42:22.160]  this domain from research to the development to early stage to late stage AI and AGI development?\n",
            "[42:22.160 --> 42:26.720]  So I think that first of all, it's really important that government's in there,\n",
            "[42:27.280 --> 42:30.400]  right? In some way, shape or form, you know, at the end of the day, we're talking about building\n",
            "[42:30.400 --> 42:37.040]  technology that will shape how the world operates and that there needs to be government as part of\n",
            "[42:37.040 --> 42:44.080]  that answer. And so that's why we've done a number of different congressional testimonies, we interact\n",
            "[42:44.080 --> 42:49.520]  with a number of different lawmakers and that, you know, right now a lot of our message to them is\n",
            "[42:49.520 --> 42:56.800]  that it's not the time for regulation, it is the time for measurement, right? That our main policy\n",
            "[42:56.800 --> 43:01.040]  recommendation is that people, and you know, the government does this all the time with bodies like\n",
            "[43:01.040 --> 43:07.840]  NIST, spend time trying to figure out just where the technology is, how fast it's moving, and can\n",
            "[43:07.840 --> 43:14.080]  really become literate and up to speed with respect to what to expect. So I think that today, the\n",
            "[43:14.080 --> 43:20.240]  answer really is about measurement, and I think that there will be a time and place where that\n",
            "[43:20.240 --> 43:26.000]  will change. And I think it's a little bit hard to predict exactly what exactly that trajectory\n",
            "[43:26.000 --> 43:32.800]  should look like. So there will be a point at which regulation, federal in the United States,\n",
            "[43:32.800 --> 43:40.000]  the government steps in and helps be the, I don't want to say the adult in the room, to make sure\n",
            "[43:40.000 --> 43:45.760]  that there is strict rules, maybe conservative rules that nobody can cross. Well, I think there's\n",
            "[43:45.760 --> 43:51.120]  kind of maybe two angles to it. So today with narrow AI applications, I think there are already\n",
            "[43:51.120 --> 43:55.280]  existing bodies that are responsible and should be responsible for regulation. You think about,\n",
            "[43:55.280 --> 44:02.960]  for example, with self-driving cars that you want the national highway to be regulating that. That\n",
            "[44:02.960 --> 44:07.360]  makes sense, right? That basically what we're saying is that we're going to have these technological\n",
            "[44:07.360 --> 44:13.520]  systems that are going to be performing applications that humans already do. Great. We already have\n",
            "[44:13.520 --> 44:17.360]  ways of thinking about standards and safety for those. So I think actually empowering those\n",
            "[44:17.360 --> 44:24.640]  regulators today is also pretty important. And then I think for AGI, there's going to be a point\n",
            "[44:24.640 --> 44:29.760]  where we'll have better answers. And I think that maybe a similar approach of first measurement and\n",
            "[44:29.760 --> 44:33.040]  start thinking about what the rules should be. I think it's really important that we don't\n",
            "[44:33.760 --> 44:40.240]  prematurely squash progress. I think it's very easy to kind of smother a budding field. And I\n",
            "[44:40.240 --> 44:44.640]  think that's something to really avoid. But I don't think the right way of doing it is to say,\n",
            "[44:44.640 --> 44:50.000]  let's just try to blaze ahead and not involve all these other stakeholders.\n",
            "[44:50.000 --> 45:00.800]  So you recently released a paper on GPT-2 language modeling, but did not release the full\n",
            "[45:01.600 --> 45:06.800]  model because you had concerns about the possible negative effects of the availability of such\n",
            "[45:06.800 --> 45:13.600]  model. Outside of just that decision, it's super interesting because of the discussion\n",
            "[45:13.600 --> 45:19.840]  at a societal level, the discourse it creates. So it's fascinating in that aspect. But if you think\n",
            "[45:19.840 --> 45:25.760]  that's the specifics here at first, what are some negative effects that you envisioned? And of\n",
            "[45:25.760 --> 45:30.400]  course, what are some of the positive effects? Yeah. So again, I think to zoom out, like the way\n",
            "[45:30.400 --> 45:37.600]  that we thought about GPT-2 is that with language modeling, we are clearly on a trajectory right now\n",
            "[45:37.600 --> 45:44.720]  where we scale up our models and we get qualitatively better performance. GPT-2 itself\n",
            "[45:44.720 --> 45:50.080]  was actually just a scale up of a model that we've released in the previous June. We just\n",
            "[45:50.080 --> 45:54.880]  ran it at much larger scale and we got these results where suddenly starting to create\n",
            "[45:54.880 --> 46:00.800]  coherent pros, which was not something we'd seen previously. And what are we doing now? Well,\n",
            "[46:00.800 --> 46:06.480]  we're going to scale up GPT-2 by 10X, by 100X, by 1000X, and we don't know what we're going to get.\n",
            "[46:06.480 --> 46:12.720]  And so it's very clear that the model that we released last June, I think it's kind of like,\n",
            "[46:12.720 --> 46:18.400]  it's a good academic toy. It's not something that we think is something that can really have\n",
            "[46:18.400 --> 46:22.240]  negative applications or to the extent that it can, that the positive of people being able to\n",
            "[46:22.240 --> 46:30.560]  play with it is far outweighs the possible harms. You fast forward to not GPT-2, but GPT-20,\n",
            "[46:30.560 --> 46:35.120]  and you think about what that's going to be like. And I think that the capabilities are going to be\n",
            "[46:35.120 --> 46:41.600]  substantive. And so there needs to be a point in between the two where you say, this is something\n",
            "[46:41.600 --> 46:46.000]  where we are drawing the line and that we need to start thinking about the safety aspects.\n",
            "[46:46.000 --> 46:49.600]  And I think for GPT-2, we could have gone either way. And in fact, when we had conversations\n",
            "[46:49.600 --> 46:55.440]  internally that we had a bunch of pros and cons and it wasn't clear which one outweighed the other.\n",
            "[46:55.440 --> 46:59.600]  And I think that when we announced that, hey, we decide not to release this model, then there was a\n",
            "[46:59.600 --> 47:02.960]  bunch of conversation where various people said, it's so obvious that you should have just released\n",
            "[47:02.960 --> 47:06.720]  it. There are other people said, it's so obvious you should not have released it. And I think that\n",
            "[47:06.720 --> 47:12.800]  that almost definitionally means that holding it back was the correct decision. If it's not obvious\n",
            "[47:12.800 --> 47:15.920]  whether something is beneficial or not, you should probably default to caution.\n",
            "[47:16.640 --> 47:22.080]  And so I think that the overall landscape for how we think about it is that this decision could have\n",
            "[47:22.080 --> 47:26.720]  gone either way. There are great arguments in both directions, but for future models down the road,\n",
            "[47:26.720 --> 47:31.040]  and possibly sooner than you'd expect, because scaling these things up doesn't actually take that\n",
            "[47:31.040 --> 47:38.000]  on, those ones, you're definitely not going to want to release into the wild. And so I think that we\n",
            "[47:38.000 --> 47:43.920]  almost view this as a test case and to see, can we even design, how do you have a society, how do you\n",
            "[47:43.920 --> 47:48.560]  have a system that goes from having no concept of responsible disclosure, where the mere idea\n",
            "[47:48.560 --> 47:53.920]  of not releasing something for safety reasons is unfamiliar to a world where you say, okay,\n",
            "[47:53.920 --> 47:57.600]  we have a powerful model. Let's at least think about it. Let's go through some process.\n",
            "[47:57.600 --> 48:01.360]  And you think about the security community, it took them a long time to design responsible\n",
            "[48:01.360 --> 48:05.840]  disclosure. You think about this question of, well, I have a security exploit. I send it to\n",
            "[48:05.840 --> 48:12.160]  the company. The company tries to prosecute me or just sit, just ignores it. What do I do?\n",
            "[48:12.160 --> 48:15.440]  And so the alternatives of, oh, I just always publish your article. I always publish your\n",
            "[48:15.440 --> 48:21.360]  exploits. That doesn't seem good either. And so it really took a long time and it was bigger than\n",
            "[48:21.360 --> 48:25.200]  any individual. It's really about building a whole community that believe that, okay, we'll have this\n",
            "[48:25.200 --> 48:29.200]  process where you send it to the company. If they don't act in a certain time, then you can go\n",
            "[48:29.200 --> 48:35.680]  public and you're not a bad person, you've done the right thing. And I think that in AI, part of\n",
            "[48:35.680 --> 48:43.360]  the response to GPT-2 just proves that we don't have any concept of this. So the question is,\n",
            "[48:43.360 --> 48:50.480]  what do we do? That's the high level picture. And so I think that this was a really important move\n",
            "[48:50.480 --> 48:55.920]  to make and we could have maybe delayed it for GPT-3, but I'm really glad we did it for GPT-2.\n",
            "[48:55.920 --> 48:59.920]  And so now you look at GPT-2 itself and you think about the substance of, okay, what are potential\n",
            "[48:59.920 --> 49:04.800]  negative applications? So you have this model that's been trained on the internet, which is\n",
            "[49:04.800 --> 49:09.040]  also going to be a bunch of very biased data, a bunch of very offensive content in there.\n",
            "[49:09.040 --> 49:14.880]  And you can ask it to generate content for you on basically any topic. You just give it a prompt and\n",
            "[49:14.880 --> 49:19.760]  it'll just start writing and it writes content like you see on the internet, even down to like\n",
            "[49:19.760 --> 49:25.040]  saying advertisement in the middle of some of its generations. And you think about the\n",
            "[49:25.040 --> 49:30.080]  possibilities for generating fake news or abusive content. And it's interesting seeing what people\n",
            "[49:30.080 --> 49:36.000]  have done with, we released a smaller version of GPT-2 and the people have done things like try to\n",
            "[49:36.000 --> 49:43.120]  generate, take my own Facebook message history and generate more Facebook messages like me and\n",
            "[49:43.120 --> 49:50.400]  people generating fake politician content. There's a bunch of things there where you at least have\n",
            "[49:50.400 --> 49:55.920]  to think, is this going to be good for the world? There's the flip side, which is I think that\n",
            "[49:55.920 --> 50:01.120]  there's a lot of awesome applications that we really want to see, like creative applications\n",
            "[50:01.120 --> 50:06.240]  in terms of if you have sci-fi authors that can work with this tool and come up with cool ideas,\n",
            "[50:06.960 --> 50:11.360]  that seems awesome if we can write better sci-fi through the use of these tools. And we've actually\n",
            "[50:11.360 --> 50:16.720]  had a bunch of people write into us asking, hey, can we use it for a variety of different creative\n",
            "[50:16.720 --> 50:26.880]  applications? So the positive are actually pretty easy to imagine. The usual NLP applications are\n",
            "[50:26.880 --> 50:33.120]  really interesting. But let's go there. It's kind of interesting to think about a world where,\n",
            "[50:33.920 --> 50:41.040]  look at Twitter, where not just fake news, but smarter and smarter bots being able to\n",
            "[50:42.480 --> 50:49.760]  spread in an interesting complex networking way information that just floods out us regular human\n",
            "[50:49.760 --> 50:58.960]  beings with our original thoughts. So what are your views of this world with GPT-20, right?\n",
            "[51:00.160 --> 51:04.640]  How do we think about it? Again, it's like one of those things about in the 50s trying to describe\n",
            "[51:05.440 --> 51:10.960]  the internet or the smartphone. What do you think about that world, the nature of information?\n",
            "[51:12.880 --> 51:18.960]  One possibility is that we'll always try to design systems that identify robot versus human\n",
            "[51:18.960 --> 51:24.480]  and we'll do so successfully. And so we'll authenticate that we're still human. And the other\n",
            "[51:24.480 --> 51:30.400]  world is that we just accept the fact that we're swimming in a sea of fake news and just learn to\n",
            "[51:30.400 --> 51:41.200]  swim there. Well, have you ever seen the popular meme of robot with a physical arm and pen clicking\n",
            "[51:41.200 --> 51:48.400]  the I'm not a robot button? Yeah. I think the truth is that really trying to distinguish between\n",
            "[51:48.400 --> 51:53.120]  robot and human is a losing battle. Ultimately, you think it's a losing battle. I think it's a\n",
            "[51:53.120 --> 51:57.120]  losing battle ultimately. I think that that is that in terms of the content, in terms of the\n",
            "[51:57.120 --> 52:01.120]  actions that you could take. I mean, think about how captures have gone. The captures used to be a\n",
            "[52:01.120 --> 52:06.800]  very nice, simple, you just have this image. All of our OCR is terrible. You put a couple of artifacts\n",
            "[52:06.800 --> 52:13.200]  in it, humans are going to be able to tell what it is. An AI system wouldn't be able to today. I\n",
            "[52:13.200 --> 52:18.480]  could barely do captures. And I think that this is just kind of where we're going. I think captures\n",
            "[52:18.480 --> 52:23.440]  were a moment in time thing. And as AI systems become more powerful, that there being human\n",
            "[52:23.440 --> 52:29.120]  capabilities that can be measured in a very easy, automated way that the AIs will not be capable\n",
            "[52:29.120 --> 52:34.320]  of. I think that's just like, it's just an increasingly hard technical battle. But it's not\n",
            "[52:34.320 --> 52:41.040]  that all hope is lost. And you think about how do we already authenticate ourselves? We have systems,\n",
            "[52:41.040 --> 52:48.080]  we have social security numbers. If you're in the US or you have ways of identifying individual\n",
            "[52:48.080 --> 52:54.800]  people and having real world identity tied to digital identity seems like a step towards\n",
            "[52:54.800 --> 52:59.360]  authenticating the source of content rather than the content itself. Now there are problems with\n",
            "[52:59.360 --> 53:04.960]  that. How can you have privacy and anonymity in a world where the only content you can really trust\n",
            "[53:04.960 --> 53:09.440]  is, or the only way you can trust content is by looking at where it comes from. And so I think\n",
            "[53:09.440 --> 53:15.280]  that building out good reputation networks may be one possible solution. But yeah, I think that this\n",
            "[53:15.280 --> 53:20.800]  question is not an obvious one. And I think that we, maybe sooner than we think we'll be in a world\n",
            "[53:20.800 --> 53:26.640]  where today I often will read a tweet and be like, do I feel like a real human wrote this? Or do I\n",
            "[53:26.640 --> 53:30.480]  feel like this is genuine? I feel like I can kind of judge the content a little bit. And I think in\n",
            "[53:30.480 --> 53:36.720]  the future, it just won't be the case. You will get, for example, the FCC comments on net neutrality.\n",
            "[53:36.720 --> 53:41.280]  It came out later that millions of those were auto-generated and that the researchers were\n",
            "[53:41.280 --> 53:45.360]  able to do various statistical techniques to do that. What do you do in a world where\n",
            "[53:46.080 --> 53:49.840]  those statistical techniques don't exist? It's just impossible to tell the difference between\n",
            "[53:49.840 --> 53:57.280]  humans and AIs. And in fact, the most persuasive arguments are written by AI, all that stuff. It's\n",
            "[53:57.280 --> 54:02.400]  not sci-fi anymore. You'll get GPT2 making a great argument for why recycling is bad for the world.\n",
            "[54:02.400 --> 54:05.760]  You got to read that and be like, huh, you're right. We are addressing different symptoms.\n",
            "[54:05.760 --> 54:12.000]  Yeah, that's quite interesting. I mean, ultimately it boils down to the physical world being the last\n",
            "[54:12.000 --> 54:17.920]  frontier of proving. So you said like basically networks of people, humans vouching for humans\n",
            "[54:17.920 --> 54:24.080]  in the physical world and somehow the authentication ends there. I mean, if I had to ask you,\n",
            "[54:25.360 --> 54:31.680]  I mean, you're way too eloquent for a human. So if I had to ask you to authenticate, like prove,\n",
            "[54:31.680 --> 54:37.440]  how do I know you're not a robot and how do you know I'm not a robot? I think that's so far,\n",
            "[54:39.680 --> 54:43.360]  in this space, this conversation we just had, the physical movements we did\n",
            "[54:44.000 --> 54:50.080]  is the biggest gap between us and AI systems is the physical manipulation. So maybe that's the\n",
            "[54:50.080 --> 54:57.040]  last frontier. Well, here's another question is, why is solving this problem important?\n",
            "[54:57.040 --> 55:01.120]  Like what aspects are really important to us? And I think that probably where we'll end up\n",
            "[55:01.120 --> 55:05.680]  is we'll hone in on what do we really want out of knowing if we're talking to a human.\n",
            "[55:06.400 --> 55:11.280]  And I think that, again, this comes down to identity. And so I think that the internet of\n",
            "[55:11.280 --> 55:16.320]  the future, I expect to be one that will have lots of agents out there that will interact with you.\n",
            "[55:16.320 --> 55:22.800]  But I think that the question of, is this a real flesh and blood human or is this an automated\n",
            "[55:22.800 --> 55:29.760]  system may actually just be less important. Let's actually go there. It's GPT-2 is\n",
            "[55:29.760 --> 55:38.640]  impressive. And let's look at GPT-20. Why is it so bad that all my friends are GPT-20?\n",
            "[55:40.560 --> 55:47.280]  Why is it so important on the internet, do you think, to interact with only human beings?\n",
            "[55:47.280 --> 55:52.880]  Why can't we live in a world where ideas can come from models trained on human data?\n",
            "[55:52.880 --> 55:56.400]  Yeah, I think this is actually a really interesting question. This comes back to the,\n",
            "[55:56.400 --> 56:01.200]  how do you even picture a world with some new technology? And I think that one thing that I\n",
            "[56:01.200 --> 56:08.000]  think is important is, let's say, honesty. And I think that if you have almost the Turing test\n",
            "[56:08.000 --> 56:13.920]  style sense of technology, you have AIs that are pretending to be humans and deceiving you.\n",
            "[56:14.480 --> 56:19.920]  I think that feels like a bad thing. I think that it's really important that we feel like we're in\n",
            "[56:19.920 --> 56:24.640]  control of our environment, that we understand who we're interacting with. And if it's an AI or a\n",
            "[56:24.640 --> 56:30.240]  human, that's not something that we're being deceived about. But I think that the flip side of,\n",
            "[56:30.240 --> 56:35.200]  can I have as meaningful of an interaction with an AI as I can with a human? Well, I actually think\n",
            "[56:35.200 --> 56:40.400]  here you can turn to sci-fi. And her, I think, is a great example of asking this very question.\n",
            "[56:41.040 --> 56:45.120]  One thing I really love about her is it really starts out almost by asking, how meaningful are\n",
            "[56:45.120 --> 56:51.360]  human virtual relationships? And then you have a human who has a relationship with an AI, and\n",
            "[56:51.360 --> 56:55.840]  that you really start to be drawn into that, right? That all of your emotional buttons get\n",
            "[56:55.840 --> 56:59.760]  triggered in the same way as if there was a real human that was on the other side of that phone.\n",
            "[57:00.320 --> 57:05.600]  And so I think that this is one way of thinking about it, is that I think that we can have\n",
            "[57:05.600 --> 57:10.160]  meaningful interactions. And that if there's a funny joke, in some sense, it doesn't really\n",
            "[57:10.160 --> 57:14.720]  matter if it was written by a human or an AI. But what you don't want, and why I think we should\n",
            "[57:14.720 --> 57:19.760]  really draw hard lines, is deception. And I think that as long as we're in a world where,\n",
            "[57:19.760 --> 57:23.760]  you know, why do we build AI systems at all, right? The reason we want to build them is to\n",
            "[57:23.760 --> 57:28.880]  enhance human lives, to make humans be able to do more things, to have humans feel more fulfilled.\n",
            "[57:28.880 --> 57:33.200]  And if we can build AI systems that do that, you know, sign me up.\n",
            "[57:33.200 --> 57:34.880]  So the process of language modeling,\n",
            "[57:37.040 --> 57:44.000]  how far do you think it takes us? Let's look at movie Her. Do you think a dialogue, natural\n",
            "[57:44.000 --> 57:48.800]  language conversation, as formulated by the Turing test, for example, do you think that process\n",
            "[57:48.800 --> 57:53.120]  could be achieved through this kind of unsupervised language modeling?\n",
            "[57:53.120 --> 57:59.040]  So I think the Turing test, in its real form, isn't just about language, right? It's really\n",
            "[57:59.040 --> 58:03.840]  about reasoning too, right? To really pass the Turing test, I should be able to teach calculus\n",
            "[58:03.840 --> 58:08.240]  to whoever's on the other side, and have it really understand calculus, and be able to,\n",
            "[58:08.240 --> 58:14.000]  you know, go and solve new calculus problems. And so I think that to really solve the Turing test,\n",
            "[58:14.000 --> 58:18.640]  we need more than what we're seeing with language models. We need some way of plugging in reasoning.\n",
            "[58:18.640 --> 58:23.760]  Now, how different will that be from what we already do? That's an open question, right?\n",
            "[58:23.760 --> 58:28.160]  Might be that we need some sequence of totally radical new ideas, or it might be that we just\n",
            "[58:28.160 --> 58:34.080]  need to kind of shape our existing systems in a slightly different way. But I think that in terms\n",
            "[58:34.080 --> 58:38.560]  of how far language modeling will go, it's already gone way further than many people would have\n",
            "[58:38.560 --> 58:42.640]  expected, right? I think that things like, and I think there's a lot of really interesting angles\n",
            "[58:42.640 --> 58:49.200]  to poke in terms of how much does GPT-2 understand physical world? Like, you know, you read a little\n",
            "[58:49.200 --> 58:54.320]  bit about fire underwater in GPT-2, so it's like, okay, maybe it doesn't quite understand what these\n",
            "[58:54.320 --> 59:00.160]  things are. But at the same time, I think that you also see various things like smoke coming from\n",
            "[59:00.160 --> 59:04.800]  flame, and you know, a bunch of these things that GPT-2, it has no body, it has no physical experience,\n",
            "[59:04.800 --> 59:13.600]  it's just statically read data. And I think that the answer is like, we don't know yet. And these\n",
            "[59:13.600 --> 59:17.840]  questions, though, we're starting to be able to actually ask them to physical systems, to real\n",
            "[59:17.840 --> 59:21.920]  systems that exist, and that's very exciting. What's your intuition? Do you think if you just\n",
            "[59:21.920 --> 59:30.320]  scale language modeling, like significantly scale, that reasoning can emerge from the same exact\n",
            "[59:30.320 --> 59:37.040]  mechanisms? I think it's unlikely that if we just scale GPT-2 that we'll have reasoning in the\n",
            "[59:37.040 --> 59:40.880]  full-fledged way. And I think that there's like, you know, so the type signature is a little bit\n",
            "[59:40.880 --> 59:46.480]  wrong, right? That like, there's something we do with, that we call thinking, right? Where we spend\n",
            "[59:46.480 --> 59:50.960]  a lot of compute, like a variable amount of compute to get to better answers, right? I think a little\n",
            "[59:50.960 --> 59:57.280]  bit harder, I get a better answer. And that that kind of type signature isn't quite encoded in\n",
            "[59:57.280 --> 1:00:03.520]  a GPT, right? GPT will kind of like, it's been a long time in its evolutionary history,\n",
            "[1:00:03.520 --> 1:00:06.960]  baking in all this information, getting very, very good at this predictive process.\n",
            "[1:00:06.960 --> 1:00:14.000]  And then at runtime, I just kind of do one forward pass and I'm able to generate stuff. And so,\n",
            "[1:00:14.000 --> 1:00:17.920]  you know, there might be small tweaks to what we do in order to get the type signature, right?\n",
            "[1:00:17.920 --> 1:00:21.520]  For example, well, you know, it's not really one forward pass, right? You know, you generate\n",
            "[1:00:21.520 --> 1:00:26.160]  symbol by symbol. And so maybe you generate like a whole sequence of thoughts and you only keep like\n",
            "[1:00:26.160 --> 1:00:30.640]  the last bit or something. But I think that at the very least, I would expect you have to make\n",
            "[1:00:30.640 --> 1:00:37.440]  changes like that. Yeah, just exactly how we, you said, think is the process of generating\n",
            "[1:00:37.440 --> 1:00:41.760]  thought by thought in the same kind of way, like you said, keep the last bit,\n",
            "[1:00:41.760 --> 1:00:47.200]  the thing that we converge towards. And I think there's another piece, which is interesting,\n",
            "[1:00:47.200 --> 1:00:52.320]  which is this out of distribution generalization, right? That like thinking somehow lets us do that,\n",
            "[1:00:52.320 --> 1:00:56.400]  right? That we haven't experienced a thing and yet somehow we just kind of keep refining our mental\n",
            "[1:00:56.400 --> 1:01:04.320]  model of it. This is again, something that feels tied to whatever reasoning is. And maybe it's a\n",
            "[1:01:04.320 --> 1:01:08.800]  small tweak to what we do. Maybe it's many ideas and will take us many decades. Yeah. So the\n",
            "[1:01:08.800 --> 1:01:14.640]  assumption there, generalization out of distribution is that it's possible to create\n",
            "[1:01:14.640 --> 1:01:21.360]  new ideas. It's possible that nobody's ever created any new ideas. And then with scaling\n",
            "[1:01:21.360 --> 1:01:30.400]  GPT-2 to GPT-20, you would essentially generalize to all possible thoughts that I assume is going to\n",
            "[1:01:30.400 --> 1:01:36.080]  happen. Just to play devil's advocate here. Right, right, right. I mean, how many new story\n",
            "[1:01:36.080 --> 1:01:41.760]  ideas have we come up with since Shakespeare, right? Yeah, exactly. It's just all different\n",
            "[1:01:41.760 --> 1:01:47.120]  forms of love and drama and so on. Okay. Not sure if you read Bit of Lesson, a recent blog\n",
            "[1:01:47.120 --> 1:01:53.520]  post by Ray Sutton. Yep, I have. He basically says something that echoes some of the ideas that\n",
            "[1:01:53.520 --> 1:01:58.880]  you've been talking about, which is he says the biggest lesson that can be read from 70 years of\n",
            "[1:01:58.880 --> 1:02:05.520]  AI research is that general methods that leverage computation are ultimately going to ultimately\n",
            "[1:02:05.520 --> 1:02:13.280]  win out. Do you agree with this? So basically, OpenAI in general, but the ideas you're exploring\n",
            "[1:02:13.280 --> 1:02:20.080]  while coming up with methods, whether it's GPT-2 modeling or whether it's OpenAI 5 playing Dota,\n",
            "[1:02:20.080 --> 1:02:30.160]  or a general method is better than a more fine-tuned, expert-tuned method. Yeah. So I think\n",
            "[1:02:30.160 --> 1:02:34.240]  that, well, one thing that I think was really interesting about the reaction to that blog post\n",
            "[1:02:34.240 --> 1:02:39.040]  was that a lot of people have read this as saying that compute is all that matters. And that's a\n",
            "[1:02:39.040 --> 1:02:43.840]  very threatening idea, right? And I don't think it's a true idea either. It's very clear that we\n",
            "[1:02:43.840 --> 1:02:47.760]  have algorithmic ideas that have been very important for making progress and to really\n",
            "[1:02:47.760 --> 1:02:52.160]  build AGI. You want to push as far as you can on the computational scale, and you want to push as\n",
            "[1:02:52.160 --> 1:02:57.280]  far as you can on human ingenuity. And so I think you need both. But I think the way that you phrase\n",
            "[1:02:57.280 --> 1:03:02.080]  the question is actually very good, right? That it's really about what kind of ideas should we be\n",
            "[1:03:02.080 --> 1:03:08.960]  striving for? And absolutely, if you can find a scalable idea, you pour more data into it,\n",
            "[1:03:08.960 --> 1:03:16.000]  it gets better. That's the real holy grail. And so I think that the answer to the question,\n",
            "[1:03:16.000 --> 1:03:21.120]  I think, is yes, that that's really how we think about it, and that part of why we're excited\n",
            "[1:03:21.120 --> 1:03:25.920]  about the power of deep learning, the potential for building AGI, is because we look at the\n",
            "[1:03:25.920 --> 1:03:32.080]  systems that exist in the most successful AI systems, and we realize that you scale those up,\n",
            "[1:03:32.080 --> 1:03:36.240]  they're going to work better. And I think that that scalability is something that really gives us\n",
            "[1:03:36.240 --> 1:03:40.800]  hope for being able to build transformative systems. So I'll tell you, this is partially an\n",
            "[1:03:40.800 --> 1:03:46.400]  emotional, you know, a thing that, a response that people often have, is compute is so important for\n",
            "[1:03:46.400 --> 1:03:51.680]  state-of-the-art performance. You know, individual developers, maybe a 13-year-old sitting somewhere\n",
            "[1:03:51.680 --> 1:03:56.160]  in Kansas or something like that, you know, they're sitting, they might not even have a GPU,\n",
            "[1:03:56.160 --> 1:04:01.840]  or may have a single GPU, a 1080 or something like that. And there's this feeling like, well,\n",
            "[1:04:01.840 --> 1:04:10.240]  how can I possibly compete or contribute to this world of AI if scale is so important? So if you\n",
            "[1:04:10.240 --> 1:04:14.880]  can comment on that, and in general, do you think we need to also in the future focus on\n",
            "[1:04:14.880 --> 1:04:22.080]  democratizing compute resources more or as much as we democratize the algorithms?\n",
            "[1:04:22.080 --> 1:04:28.320]  Well, so the way that I think about it is that there's this space of possible progress, right?\n",
            "[1:04:28.320 --> 1:04:32.960]  There's a space of ideas and sort of systems that will work, that will move us forward. And there's\n",
            "[1:04:32.960 --> 1:04:37.360]  a portion of that space, and to some extent, an increasingly significant portion of that space,\n",
            "[1:04:37.360 --> 1:04:43.920]  that does just require massive compute resources. And for that, I think that the answer is kind of\n",
            "[1:04:43.920 --> 1:04:48.320]  clear and that part of why we have the structure that we do is because we think it's really\n",
            "[1:04:48.320 --> 1:04:52.720]  important to be pushing the scale and to be, you know, building these large clusters and systems.\n",
            "[1:04:53.280 --> 1:04:57.360]  But there's another part portion of the space that isn't about the large-scale compute, that\n",
            "[1:04:57.360 --> 1:05:02.800]  are these ideas that, and again, I think that for the ideas to really be impactful and really shine,\n",
            "[1:05:02.800 --> 1:05:06.880]  that they should be ideas that if you scale them up, would work way better than they do at small\n",
            "[1:05:06.880 --> 1:05:12.560]  scale, but that you can discover them without massive computational resources. And if you look\n",
            "[1:05:12.560 --> 1:05:17.600]  at the history of recent developments, you think about things like the GAN or the VAE,\n",
            "[1:05:17.600 --> 1:05:22.240]  that these are ones that I think you could come up with them without having, and you know,\n",
            "[1:05:22.240 --> 1:05:26.400]  in practice, people did come up with them without having massive, massive computational resources.\n",
            "[1:05:26.400 --> 1:05:32.560]  Right, I just talked to Ian Goodfellow, but the thing is, the initial GAN produced pretty terrible\n",
            "[1:05:32.560 --> 1:05:38.160]  results, right? So only because it was in a very specific, it was only because they're smart enough\n",
            "[1:05:38.160 --> 1:05:43.760]  to know that this is quite surprising, it can generate anything that they know. Do you see a\n",
            "[1:05:43.760 --> 1:05:50.240]  world, or is that too optimistic and dreamer-like to imagine that the compute resources are something\n",
            "[1:05:50.240 --> 1:05:56.400]  that's owned by governments and provided as utility? Actually, to some extent, this question\n",
            "[1:05:56.400 --> 1:06:02.320]  reminds me of a blog post from one of my former professors at Harvard, this guy, Matt Welsh,\n",
            "[1:06:02.320 --> 1:06:06.560]  who was a systems professor. I remember sitting in his 10-year talk, right, and, you know, that\n",
            "[1:06:06.560 --> 1:06:13.360]  he had literally just gotten tenure. He went to Google for the summer, and then decided he wasn't\n",
            "[1:06:13.360 --> 1:06:18.720]  going back to academia, right? And kind of in his blog post, he makes this point that, look,\n",
            "[1:06:18.720 --> 1:06:23.680]  as a systems researcher, that I come up with these cool system ideas, right, and I kind of build a\n",
            "[1:06:23.680 --> 1:06:29.280]  little proof of concept, and the best thing I can hope for is that the people at Google or Yahoo,\n",
            "[1:06:30.000 --> 1:06:35.600]  which was around at the time, will implement it and actually make it work at scale, right? That's\n",
            "[1:06:35.600 --> 1:06:38.480]  like the dream for me, right? I built a little thing, and they turned it into a big thing that's\n",
            "[1:06:38.480 --> 1:06:44.880]  actually working. And for him, he said, I'm done with that. I want to be the person who's actually\n",
            "[1:06:44.880 --> 1:06:49.760]  doing the building and deploying. And I think that there's a similar dichotomy here, right? I think\n",
            "[1:06:49.760 --> 1:06:55.120]  that there are people who really actually find value, and I think it is a valuable thing to do\n",
            "[1:06:55.120 --> 1:06:59.200]  to be the person who produces those ideas, right, who builds the proof of concept. And yeah, you\n",
            "[1:06:59.200 --> 1:07:05.120]  don't get to generate the coolest possible GAN images, but you invented the GAN, right? And so\n",
            "[1:07:05.120 --> 1:07:08.960]  that there's a real trade-off there. And I think that that's a very personal choice,\n",
            "[1:07:08.960 --> 1:07:16.320]  but I think there's value in both sides. So do you think creating AGI, something or some new models,\n",
            "[1:07:18.240 --> 1:07:23.520]  we would see echoes of the brilliance even at the prototype level? So you would be able to develop\n",
            "[1:07:23.520 --> 1:07:29.280]  those ideas without scale, the initial seeds? Well, so take a look at, you know, I always like\n",
            "[1:07:29.280 --> 1:07:34.480]  to look at examples that exist, right? Look at real precedent. And so take a look at the\n",
            "[1:07:34.480 --> 1:07:40.240]  June 2018 model that we released that we scaled up to turn into GPT-2. And you can see that at\n",
            "[1:07:40.240 --> 1:07:46.160]  small scale, it set some records, right? This was the original GPT. We actually had some cool\n",
            "[1:07:46.160 --> 1:07:51.440]  generations that weren't nearly as amazing and really stunning as the GPT-2 ones, but it was\n",
            "[1:07:51.440 --> 1:07:56.160]  promising. It was interesting. And so I think it is the case that with a lot of these ideas that\n",
            "[1:07:56.160 --> 1:08:01.120]  you see promise at small scale. But there is an asterisk here, a very big asterisk, which is\n",
            "[1:08:01.120 --> 1:08:08.000]  sometimes we see behaviors that emerge that are qualitatively different from anything we saw at\n",
            "[1:08:08.000 --> 1:08:13.920]  small scale. And that the original inventor of whatever algorithm looks at and says, I didn't\n",
            "[1:08:13.920 --> 1:08:19.200]  think it could do that. This is what we saw in Dota, right? So PPO was created by John Shulman,\n",
            "[1:08:19.200 --> 1:08:25.920]  who's a researcher here. And with Dota, we basically just ran PPO at massive, massive scale.\n",
            "[1:08:25.920 --> 1:08:32.160]  And there's some tweaks in order to make it work, but fundamentally it's PPO at the core. And we\n",
            "[1:08:32.160 --> 1:08:38.240]  were able to get this long-term planning, these behaviors to really play out on a time scale that\n",
            "[1:08:38.240 --> 1:08:42.880]  we just thought was not possible. And John looked at that and was like, I didn't think it could do\n",
            "[1:08:42.880 --> 1:08:47.200]  that. That's what happens when you're at three orders of magnitude more scale than you tested at.\n",
            "[1:08:47.760 --> 1:08:54.400]  Yeah, but it still has the same flavors of, at least echoes of the expected billions.\n",
            "[1:08:54.400 --> 1:09:00.880]  Although I suspect with GPT scaled more and more, you might get surprising things. So yeah,\n",
            "[1:09:00.880 --> 1:09:07.200]  you're right. It's interesting. It's difficult to see how far an idea will go when it's scaled.\n",
            "[1:09:07.760 --> 1:09:12.960]  It's an open question. Well, so to that point with Dota and PPO, like, I mean, here's a very concrete\n",
            "[1:09:12.960 --> 1:09:16.400]  one, right? It's like, it's actually one thing that's very surprising about Dota that I think\n",
            "[1:09:16.400 --> 1:09:21.520]  people don't really pay that much attention to is the decree of generalization out of distribution\n",
            "[1:09:21.520 --> 1:09:27.040]  that happens, right? That you have this AI that's trained against other bots for its entirety,\n",
            "[1:09:27.040 --> 1:09:33.360]  the entirety of its existence. Sorry to take a step back. Can you talk through in, you know,\n",
            "[1:09:34.320 --> 1:09:41.680]  a story of Dota, a story of leading up to opening AI 5 and that past and what was the process of\n",
            "[1:09:41.680 --> 1:09:48.320]  self-planning and so on of training? Yeah. Yeah. Yeah. So with Dota, Dota is a complex video game\n",
            "[1:09:48.320 --> 1:09:52.640]  and we started training, we started trying to solve Dota because we felt like this was a step\n",
            "[1:09:52.640 --> 1:09:57.120]  towards the real world relative to other games like chess or go, right? Those very cerebral\n",
            "[1:09:57.120 --> 1:10:00.960]  games where you just kind of have this board, very discreet moves. Dota starts to be much more\n",
            "[1:10:00.960 --> 1:10:05.520]  continuous time that you have this huge variety of different actions that you have a 45 minute\n",
            "[1:10:05.520 --> 1:10:10.880]  game with all these different units. And it's got a lot of messiness to it that really hasn't been\n",
            "[1:10:10.880 --> 1:10:16.320]  captured by previous games. And famously all of the hardcoded bots for Dota were terrible.\n",
            "[1:10:16.320 --> 1:10:20.400]  Right? It's just impossible to write anything good for it because it's so complex. And so this seems\n",
            "[1:10:20.400 --> 1:10:25.680]  like a really good place to push what's the state of the art in reinforcement learning. And so we\n",
            "[1:10:25.680 --> 1:10:30.400]  started by focusing on the one versus one version of the game and, and we're able to solve that.\n",
            "[1:10:30.400 --> 1:10:34.800]  We were able to beat the world champions and the, the, the, the learning, you know, the, the skill\n",
            "[1:10:34.800 --> 1:10:39.120]  curve was this crazy exponential, right? And it was like constantly we were just scaling up that\n",
            "[1:10:39.120 --> 1:10:43.200]  we were fixing bugs and you know, that you look at the, at the skill curve and it was really a\n",
            "[1:10:43.200 --> 1:10:47.360]  very, very smooth one. This is actually really interesting to see how that like human iteration\n",
            "[1:10:47.360 --> 1:10:53.360]  loop yielded very steady exponential progress. And, uh, to one side note, first of all, it's an\n",
            "[1:10:53.360 --> 1:10:58.960]  exceptionally popular video game. The side effect is that there's a lot of incredible human experts\n",
            "[1:10:58.960 --> 1:11:03.680]  at that video game. So the benchmark that you're trying to reach is very high. And the other,\n",
            "[1:11:03.680 --> 1:11:09.360]  can you talk about the approach that was used initially and throughout training these agents to\n",
            "[1:11:09.360 --> 1:11:14.000]  play this game? Yup. And so the approach that we used is self play. And so you have two agents that\n",
            "[1:11:14.000 --> 1:11:18.960]  don't know anything. They battle each other, they discover something a little bit good and now they\n",
            "[1:11:18.960 --> 1:11:22.640]  both know it and they just get better and better and better without bound. And that's a really\n",
            "[1:11:22.640 --> 1:11:28.720]  powerful idea, right? Uh, that we then went from the one versus one version of the game and scaled\n",
            "[1:11:28.720 --> 1:11:32.400]  up to five versus five, right? So you think about kind of like with basketball where you have this\n",
            "[1:11:32.400 --> 1:11:37.200]  like team sport and you need to do all this coordination. Um, and we were able to push the\n",
            "[1:11:37.200 --> 1:11:44.720]  same idea, the same self play, uh, to, to, to, to really get to the professional level at the full\n",
            "[1:11:44.720 --> 1:11:50.000]  five versus five version of the game. And, uh, and, and the things I think are really interesting\n",
            "[1:11:50.000 --> 1:11:54.720]  here is that these agents, in some ways they're almost like an insect like intelligence, right?\n",
            "[1:11:54.720 --> 1:11:58.080]  Where that, you know, there's, they have a lot in common with how an insect is trained, right?\n",
            "[1:11:58.080 --> 1:12:02.320]  Insect kind of lives in this environment for a very long time, or, you know, the ancestors of\n",
            "[1:12:02.320 --> 1:12:05.760]  this insect have been around for a long time and had a lot of experience with this kind of\n",
            "[1:12:05.760 --> 1:12:10.160]  environment for a long time and had a lot of experience that gets baked into, into, into this\n",
            "[1:12:10.160 --> 1:12:14.720]  agent. And, you know, it's not really smart in the sense of a human, right? It's not able to go and\n",
            "[1:12:14.720 --> 1:12:18.720]  learn calculus, but it's able to navigate its environment extremely well. It's able to handle\n",
            "[1:12:18.720 --> 1:12:23.520]  unexpected things in the environment that it's never seen before pretty well. Um, and we see\n",
            "[1:12:23.520 --> 1:12:27.920]  the same sort of thing with our Dota bots, right? That they're able to in within this game, they're\n",
            "[1:12:27.920 --> 1:12:32.480]  able to play against humans, which is something that never existed in its evolutionary environment,\n",
            "[1:12:32.480 --> 1:12:36.720]  play styles from humans versus the bots. And yet it's able to handle it extremely well.\n",
            "[1:12:37.280 --> 1:12:42.240]  And that's something that I think was very surprising to us was something that doesn't\n",
            "[1:12:42.240 --> 1:12:47.920]  really emerge from what we've seen with PPO at smaller scale, right? And the kind of scale we're\n",
            "[1:12:47.920 --> 1:12:52.480]  running this stuff at was, uh, you know, I could take a hundred thousand CPU cores running with\n",
            "[1:12:52.480 --> 1:12:57.280]  like hundreds of GPUs. Uh, it was probably about, uh, you know, like, you know, something like\n",
            "[1:12:57.280 --> 1:13:05.440]  hundreds of, of years of experience going into this bot every single real day. And so that scale\n",
            "[1:13:05.440 --> 1:13:09.600]  is massive. And we start to see very different kinds of behaviors out of the algorithms that we\n",
            "[1:13:09.600 --> 1:13:18.080]  all know and love. Dota, you mentioned beat the world expert one V one. And then, uh, you didn't\n",
            "[1:13:18.080 --> 1:13:24.960]  weren't able to win five view five this year at the best players in the world. Uh, so what's,\n",
            "[1:13:24.960 --> 1:13:29.520]  what's the comeback story? What's first of all, talk through that exceptionally exciting event\n",
            "[1:13:29.520 --> 1:13:34.160]  and uh, what's, what's the following months in this year look like? Yeah. Yeah. So, well,\n",
            "[1:13:34.160 --> 1:13:40.000]  one thing that's interesting is that, uh, you know, we lose all the time because we play,\n",
            "[1:13:40.000 --> 1:13:45.360]  so the Dota team at open AI, we, we play the bot against better players than our system all the\n",
            "[1:13:45.360 --> 1:13:50.080]  time. Or at least we used to, right? Like, you know, the, the, the first time we lost publicly\n",
            "[1:13:50.080 --> 1:13:54.160]  was we went up on stage at the international and we played against some of the best teams in the\n",
            "[1:13:54.160 --> 1:13:58.720]  world. Um, and we ended up losing both games, but we gave them a run for their money, right? The\n",
            "[1:13:58.720 --> 1:14:03.200]  both games were kind of 30 minutes, 25 minutes, and they went back and forth, back and forth,\n",
            "[1:14:03.200 --> 1:14:08.720]  back and forth. And so I think that really shows that we're at the professional level. Um, and that\n",
            "[1:14:08.720 --> 1:14:12.560]  kind of looking at those games, we think that the coin could have gone a different direction and we\n",
            "[1:14:12.560 --> 1:14:16.720]  could have, could have had some wins. That was actually very encouraging for us. Um, and you know,\n",
            "[1:14:16.720 --> 1:14:21.520]  it's interesting because the international was at a fixed time, right? So we, we knew exactly what\n",
            "[1:14:21.520 --> 1:14:25.920]  day we were going to be playing and we pushed as far as we could as fast as we could. Two weeks\n",
            "[1:14:25.920 --> 1:14:30.400]  later, we had a bot that had an 80% win rate versus the one that played at TI. Um, so the\n",
            "[1:14:30.400 --> 1:14:34.800]  march of progress, uh, you know, you should think of it as a snapshot rather than as an end state.\n",
            "[1:14:34.800 --> 1:14:39.200]  Um, and so in fact, we'll, we'll be announcing our, uh, our, our finals, uh, pretty soon. I\n",
            "[1:14:39.200 --> 1:14:44.560]  actually think that, uh, we'll announce our final match, uh, prior to, uh, this podcast being\n",
            "[1:14:44.560 --> 1:14:48.960]  released. So there should be, uh, we'll be playing, we'll be playing, uh, against the,\n",
            "[1:14:48.960 --> 1:14:54.000]  the world champions. And, you know, for us, it's really less about like the way that we think about\n",
            "[1:14:54.000 --> 1:15:00.160]  what's upcoming is the final milestone, the file competitive milestone for the project,\n",
            "[1:15:00.160 --> 1:15:06.000]  right? That our goal in all of this isn't really about beating humans at Dota. Our goal is to\n",
            "[1:15:06.000 --> 1:15:09.280]  push the state of the art and reinforcement learning. And we've done that, right? And we've\n",
            "[1:15:09.280 --> 1:15:13.360]  actually learned a lot from our system and that we have, uh, you know, I think a lot of exciting\n",
            "[1:15:13.360 --> 1:15:17.360]  next steps that we want to take. And so, you know, kind of as a final showcase of what we built,\n",
            "[1:15:17.360 --> 1:15:21.040]  we're going to do this match. Um, but for us, it's not really the success or failure,\n",
            "[1:15:21.040 --> 1:15:24.960]  um, to see, you know, do, do, do we have the coin flip go in our direction or against\n",
            "[1:15:26.000 --> 1:15:32.880]  where do you see the field of deep learning heading in the next few years? Uh, where do you see the\n",
            "[1:15:34.160 --> 1:15:40.480]  work and reinforcement learning perhaps heading and, uh, more specifically with open AI,\n",
            "[1:15:40.480 --> 1:15:46.880]  uh, all the exciting projects that you're working on? What does 2019 hold for you? Massive scale\n",
            "[1:15:46.880 --> 1:15:51.280]  scale. I will put an actress on that and just say, you know, I think that it's about ideas plus\n",
            "[1:15:51.280 --> 1:15:57.280]  scale. You need both. So that's a really good, uh, point. So the question in terms of ideas,\n",
            "[1:15:57.920 --> 1:16:04.480]  you have a lot of projects that are exploring different areas of intelligence. And, uh,\n",
            "[1:16:04.480 --> 1:16:09.360]  the question is when you, when you think of scale, do you think about growing the scale of those\n",
            "[1:16:09.360 --> 1:16:14.880]  individual projects? Or do you think about adding new projects and, uh, sorry to that. And if you\n",
            "[1:16:14.880 --> 1:16:19.920]  were thinking of adding new projects or if you look at the past, what's the process of coming\n",
            "[1:16:19.920 --> 1:16:25.280]  up with new projects and new ideas? So we really have a life cycle of project here. Uh, so we start\n",
            "[1:16:25.280 --> 1:16:29.120]  with a few people, uh, just working on a small scale idea and language is actually a very good\n",
            "[1:16:29.120 --> 1:16:33.600]  example of this, that it was really, you know, one person here who was pushing on language for a long\n",
            "[1:16:33.600 --> 1:16:38.320]  time. I mean, then you get signs of life, right? And so this is like, let's say, uh, you know, with,\n",
            "[1:16:38.320 --> 1:16:43.760]  with the original GPT, we had something that was interesting and we said, okay, it's time to scale\n",
            "[1:16:43.760 --> 1:16:47.920]  this, right? It's time to put more people on it, put more computational resources behind it. And,\n",
            "[1:16:47.920 --> 1:16:52.080]  uh, and then we just kind of keep pushing and keep pushing. And the end state is something that\n",
            "[1:16:52.080 --> 1:16:56.480]  looks like Dota or robotics, where you have a large team of, you know, 10 or 15 people, uh,\n",
            "[1:16:56.480 --> 1:17:00.560]  that are running things at very large scale, uh, and that you're able to really have material\n",
            "[1:17:00.560 --> 1:17:06.000]  engineering, uh, and, and, uh, and, and, you know, sort of machine learning science coming together\n",
            "[1:17:06.000 --> 1:17:10.800]  to make systems that work, uh, and get material results that just would have been impossible\n",
            "[1:17:10.800 --> 1:17:15.200]  otherwise. Um, so we do that whole life cycle. We've done it a number of times, uh, you know,\n",
            "[1:17:15.200 --> 1:17:20.480]  typically end to end, it's probably two, uh, two years or so, uh, to do it. I know the organization\n",
            "[1:17:20.480 --> 1:17:24.000]  has been around for three years, so maybe we'll find that we also have longer life cycle projects.\n",
            "[1:17:24.000 --> 1:17:30.240]  Um, but you know, we, we, uh, we'll work up to those. We have, uh, so, so one, one team that\n",
            "[1:17:30.240 --> 1:17:34.080]  we were actually just starting, Ilya and I are kicking off a new team called the reasoning team,\n",
            "[1:17:34.080 --> 1:17:39.360]  and that this is to really try to tackle how do you get neural networks to reason, uh, and, uh,\n",
            "[1:17:39.360 --> 1:17:43.760]  we think that this will be a long term project, uh, and it's one that we're very excited about.\n",
            "[1:17:43.760 --> 1:17:51.520]  Uh, in terms of reasoning, super exciting topic. What do you, what kind of benchmarks, uh, what\n",
            "[1:17:51.520 --> 1:17:57.920]  kind of tests of reasoning would you envision? What, what would, if you sat back, uh, with\n",
            "[1:17:57.920 --> 1:18:02.720]  whatever drink and you would be impressed that this system is able to do something,\n",
            "[1:18:02.720 --> 1:18:08.880]  what would that look like? Uh, theorem proving theorem proving. So some kind of logic and\n",
            "[1:18:08.880 --> 1:18:12.480]  especially mathematical logic. I think so. Right. And I think that there's, there's,\n",
            "[1:18:12.480 --> 1:18:15.920]  there's kind of other problems that are dual to theorem proving in particular. Um, you know,\n",
            "[1:18:15.920 --> 1:18:20.960]  you think about, uh, programming, I think about even like security analysis of, of code,\n",
            "[1:18:20.960 --> 1:18:26.560]  um, that these all kind of capture the same sorts of core reasoning and being able to do some out of\n",
            "[1:18:26.560 --> 1:18:33.040]  distribution generalization. It would be quite exciting if open AI reasoning team was able to\n",
            "[1:18:33.040 --> 1:18:38.480]  prove that P equals NP. That would be very nice. It would be very, very, very exciting, especially\n",
            "[1:18:38.480 --> 1:18:45.120]  if it turns out that P equals NP, that'll be interesting too. It just, it would be, uh, ironic\n",
            "[1:18:45.120 --> 1:18:53.440]  and humorous. Uh, so what problem stands out to you as, uh, the most exciting and challenging\n",
            "[1:18:53.440 --> 1:18:58.960]  impactful to the work for us as a community in general and for open AI this year? You mentioned\n",
            "[1:18:58.960 --> 1:19:02.640]  reasoning. I think that's, that's a heck of a problem. Yeah. So I think reasoning is an important\n",
            "[1:19:02.640 --> 1:19:06.240]  one. I think it's going to be hard to get good results in 2019. Um, you know, again, just like\n",
            "[1:19:06.240 --> 1:19:11.120]  we think about the life cycle takes time. Um, I think for 2019 language modeling seems to be kind\n",
            "[1:19:11.120 --> 1:19:15.760]  of on that ramp, right? It's at the point that we have a technique that works. We want to scale 100\n",
            "[1:19:15.760 --> 1:19:20.640]  X, a thousand X, see what happens. Awesome. Do you think we're living in a simulation?\n",
            "[1:19:20.640 --> 1:19:25.040]  Uh, I think it's, I think it's hard to have a real opinion about it. I, you know, it's actually\n",
            "[1:19:25.040 --> 1:19:30.000]  interesting. I separate out things that I think can have like, you know, yield materially different\n",
            "[1:19:30.000 --> 1:19:34.400]  predictions about the world, um, from ones that are just kind of, you know, fun, fun to speculate\n",
            "[1:19:34.400 --> 1:19:39.360]  about. And I kind of view simulation as more like, is there a flying teapot between Mars and Jupiter?\n",
            "[1:19:39.360 --> 1:19:44.640]  Like maybe, but it's a little bit hard to know what that would mean for my life. So there is\n",
            "[1:19:44.640 --> 1:19:50.960]  something actionable. So some of the best work OpenAI has done is in the field of reinforcement\n",
            "[1:19:50.960 --> 1:19:57.040]  learning. And, uh, some of the success of reinforcement learning come from being able to\n",
            "[1:19:57.040 --> 1:20:02.960]  simulate the problem you're trying to solve. So it, do you have a hope for reinforcement for the\n",
            "[1:20:02.960 --> 1:20:07.040]  future of reinforcement learning and for the future of simulation? Like whether it's, we're talking\n",
            "[1:20:07.040 --> 1:20:12.640]  about autonomous vehicles or any kind of system, do you see that scaling to where we'll be able to\n",
            "[1:20:12.640 --> 1:20:18.800]  simulate systems and, and hence be able to create a simulator that echoes our real world and, uh,\n",
            "[1:20:18.800 --> 1:20:22.720]  proving once and for all, even though you're denying it that we're living in a simulation.\n",
            "[1:20:23.920 --> 1:20:27.200]  I feel like it's two separate questions, right? So, you know, kind of at the core there of like,\n",
            "[1:20:27.200 --> 1:20:31.680]  can we, can we use simulation for self-driving cars? Um, take a look at our robotic system,\n",
            "[1:20:31.680 --> 1:20:37.840]  Dactyl, right? That was trained in simulation using the Dota system in fact, and it transfers to a\n",
            "[1:20:37.840 --> 1:20:42.640]  physical robot. And I think everyone looks at our Dota system, they're like, okay, it's just a game.\n",
            "[1:20:42.640 --> 1:20:45.840]  How are you ever going to escape to the real world? And the answer is, well, we did it with\n",
            "[1:20:45.840 --> 1:20:49.600]  the physical robot that no one could program. And so I think the answer is simulation goes a lot\n",
            "[1:20:49.600 --> 1:20:54.160]  further than you think, um, if you apply the right techniques to it. Um, now there's a question of,\n",
            "[1:20:54.160 --> 1:20:57.600]  you know, are the beings in that simulation going to, going to wake up and have consciousness?\n",
            "[1:20:57.600 --> 1:21:02.960]  Um, I think that one seems a lot, a lot harder to, to again, reason about. I think that, you know,\n",
            "[1:21:02.960 --> 1:21:07.040]  you really should think about like, where, where exactly does human consciousness come from?\n",
            "[1:21:07.040 --> 1:21:10.720]  And our own self-awareness and, you know, is it just that like, once you have like a complicated\n",
            "[1:21:10.720 --> 1:21:16.160]  enough neural net, do you have to worry about the agents feeling pain? Um, and, uh, you know,\n",
            "[1:21:16.160 --> 1:21:20.240]  I think there's like interesting speculation to do there, but, uh, but you know, I, again,\n",
            "[1:21:20.240 --> 1:21:24.000]  I think it's a little bit hard to know for sure. Well, let me just keep with the speculation.\n",
            "[1:21:24.000 --> 1:21:31.040]  Do you think, uh, to create intelligence, general intelligence, you need one consciousness and two,\n",
            "[1:21:31.040 --> 1:21:35.520]  a body. Do you think any of those elements are needed or is intelligence something that's\n",
            "[1:21:35.520 --> 1:21:39.760]  that's orthogonal to those? I'll stick to the kind of like the, the non-grand answer first,\n",
            "[1:21:39.760 --> 1:21:43.760]  right? So the non-grand answer is just to look at, you know, what are we already making work?\n",
            "[1:21:43.760 --> 1:21:47.440]  You'll get GPT-2. A lot of people would have said that to even get these kinds of results,\n",
            "[1:21:47.440 --> 1:21:51.920]  you need real world experience. You need a body, you need grounding. How are you supposed to reason\n",
            "[1:21:51.920 --> 1:21:55.280]  about any of these things? How are you supposed to like, even kind of know about smoke and fire\n",
            "[1:21:55.280 --> 1:22:00.400]  and those things if you've never experienced them? And GPT-2 shows that you can actually go way\n",
            "[1:22:00.400 --> 1:22:08.080]  further than that kind of reasoning would predict. So I think that the, the, in terms of doing a\n",
            "[1:22:08.080 --> 1:22:11.600]  consciousness, do we need a body? It seems the answer is probably not right. That we could\n",
            "[1:22:11.600 --> 1:22:15.840]  probably just continue to push kind of the systems we have. They already feel general.\n",
            "[1:22:15.840 --> 1:22:21.440]  Um, they're not as competent or as general or able to learn as quickly as an AGI would, but,\n",
            "[1:22:21.440 --> 1:22:27.120]  you know, they're at least like kind of proto AGI in some way and they don't need any of those things.\n",
            "[1:22:27.120 --> 1:22:32.160]  Now, now let's move to the grand answer, which is, you know, if our, our neural nets,\n",
            "[1:22:32.160 --> 1:22:36.960]  nets conscious already, would we ever know? How can we tell? Right. And, you know, here,\n",
            "[1:22:36.960 --> 1:22:41.920]  here's where the speculation starts to become, become, you know, at least interesting or fun,\n",
            "[1:22:41.920 --> 1:22:46.720]  and maybe a little bit disturbing it depending on where you take it. But it certainly seems that\n",
            "[1:22:46.720 --> 1:22:50.960]  when we think about animals, that there's some continuum of consciousness. You know, my cat,\n",
            "[1:22:50.960 --> 1:22:54.640]  I think is, uh, is conscious in some way, right? Uh, you know, for example,\n",
            "[1:22:54.640 --> 1:22:58.960]  uh, is conscious in some way, right? Uh, you know, not as conscious as a human. And you could\n",
            "[1:22:58.960 --> 1:23:02.160]  imagine that you could build a little consciousness meter, right? You pointed out a cat, it gives you\n",
            "[1:23:02.160 --> 1:23:06.960]  a little reading, pointed out a human, it gives you much bigger reading. What would happen if you\n",
            "[1:23:06.960 --> 1:23:12.080]  pointed one of those at a Dota neural net? And if you're training in this massive simulation,\n",
            "[1:23:12.080 --> 1:23:18.240]  do the neural nets feel pain? You know, it becomes pretty hard to know that the answer is no.\n",
            "[1:23:18.240 --> 1:23:23.920]  Um, and it becomes pretty hard to, to really think about what that would mean if the answer were yes.\n",
            "[1:23:25.360 --> 1:23:30.160]  And it's very possible, you know, for example, you could imagine that maybe the reason that humans\n",
            "[1:23:30.160 --> 1:23:35.200]  are have consciousness is because it's a, it's a convenient computational shortcut, right? If you\n",
            "[1:23:35.200 --> 1:23:39.920]  think about it, if you have a being that wants to avoid pain, which seems pretty important to survive\n",
            "[1:23:39.920 --> 1:23:45.120]  in this environment, um, and wants to like, you know, eat food, um, then that may be the best way\n",
            "[1:23:45.120 --> 1:23:49.120]  of doing it is to have a being that's conscious, right? That, you know, in order to succeed in the\n",
            "[1:23:49.120 --> 1:23:53.040]  environment, you need to have those properties and how are you supposed to implement them? And maybe\n",
            "[1:23:53.040 --> 1:23:57.840]  this, this consciousness is way of doing that. If that's true, then actually maybe we should expect\n",
            "[1:23:57.840 --> 1:24:02.480]  that really competent reinforcement learning agents will also have consciousness. But you know,\n",
            "[1:24:02.480 --> 1:24:05.440]  that's a big if, and I think there are a lot of other arguments that you can make in other\n",
            "[1:24:05.440 --> 1:24:11.440]  directions. I think that's a really interesting idea that even GPT-2 has some degree of consciousness.\n",
            "[1:24:11.440 --> 1:24:16.000]  That's something, uh, is actually not as crazy to think about. It's useful to think about\n",
            "[1:24:16.560 --> 1:24:20.880]  as, as we think about what it means to create intelligence of a dog, intelligence of a cat,\n",
            "[1:24:22.560 --> 1:24:26.000]  and the intelligence of a human. So last question, do you think\n",
            "[1:24:27.920 --> 1:24:33.600]  we will ever fall in love, like in the movie Her, with an artificial intelligence system\n",
            "[1:24:34.400 --> 1:24:40.960]  or an artificial intelligence system falling in love with a human? I hope so. If there's any better\n",
            "[1:24:40.960 --> 1:24:46.640]  way to end it is on love. So Greg, thanks so much for talking today. Thank you for having me.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/bIrEM2FbOLU.txt**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/bIrEM2FbOLU.vtt**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Transcript file created: /content/drive/My Drive/Colab Notebooks/Whisper Youtube/bIrEM2FbOLU.srt**"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown # **Run the model** 🚀\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** ⚙️\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"English\" #@param ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba']\n",
        "#@markdown > Language spoken in the audio, use `Auto detection` to let Whisper detect the language.\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_type = 'All' #@param ['All', '.txt', '.vtt', '.srt']\n",
        "#@markdown > Type of file to generate to record the transcription.\n",
        "#@markdown ---\n",
        "task = 'transcribe' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <br/>\n",
        "\n",
        "#@markdown ### **Fine tunning**\n",
        "#@markdown ---\n",
        "temperature = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"en\"\n",
        "\n",
        "video_transcription = whisper.transcribe(\n",
        "    whisper_model,\n",
        "    str(video_path_local),\n",
        "    temperature=temperature,\n",
        "    **args,\n",
        ")\n",
        "\n",
        "# Save output\n",
        "writing_lut = {\n",
        "    '.txt': whisper.utils.write_txt,\n",
        "    '.vtt': whisper.utils.write_vtt,\n",
        "    '.srt': whisper.utils.write_txt,\n",
        "}\n",
        "\n",
        "if output_type == \"All\":\n",
        "    for suffix, write_suffix in writing_lut.items():\n",
        "        transcript_local_path = video_path_local.with_suffix(suffix)\n",
        "        with open(transcript_local_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            write_suffix(video_transcription[\"segments\"], file=f)\n",
        "        try:\n",
        "            transcript_drive_path = drive_whisper_path / transcript_local_path.name\n",
        "            shutil.copy(transcript_local_path, transcript_drive_path)\n",
        "            display(Markdown(f\"**Transcript file created: {transcript_drive_path}**\"))\n",
        "        except:\n",
        "            display(Markdown(f\"**Transcript file created: {transcript_local_path}**\"))\n",
        "else:\n",
        "    transcript_local_path = video_path_local.with_suffix(output_type)\n",
        "\n",
        "    with open(transcript_local_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        writing_lut[output_type](video_transcription[\"segments\"], file=f)\n",
        "    try:\n",
        "        transcript_drive_path = drive_whisper_path / transcript_local_path.name\n",
        "        shutil.copy(transcript_local_path, transcript_drive_path)\n",
        "        display(Markdown(f\"**Transcript file created: {transcript_drive_path}**\"))\n",
        "    except:\n",
        "        display(Markdown(f\"**Transcript file created: {transcript_local_path}**\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ad6n1m4deAHp"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}